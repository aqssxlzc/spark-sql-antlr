# Generated from Sparksql.g4 by ANTLR 4.5.1
# encoding: utf-8
from antlr4 import *
from io import StringIO

def serializedATN():
    with StringIO() as buf:
        buf.write("\3\u0430\ud6d1\u8206\uad2d\u4417\uaef1\u8d80\uaadd\3\u0151")
        buf.write("\u0298\4\2\t\2\4\3\t\3\4\4\t\4\4\5\t\5\4\6\t\6\4\7\t\7")
        buf.write("\4\b\t\b\4\t\t\t\4\n\t\n\4\13\t\13\4\f\t\f\4\r\t\r\4\16")
        buf.write("\t\16\4\17\t\17\4\20\t\20\4\21\t\21\4\22\t\22\4\23\t\23")
        buf.write("\4\24\t\24\4\25\t\25\4\26\t\26\4\27\t\27\4\30\t\30\4\31")
        buf.write("\t\31\4\32\t\32\4\33\t\33\4\34\t\34\4\35\t\35\4\36\t\36")
        buf.write("\4\37\t\37\4 \t \4!\t!\4\"\t\"\4#\t#\4$\t$\4%\t%\4&\t")
        buf.write("&\4\'\t\'\4(\t(\4)\t)\4*\t*\4+\t+\4,\t,\4-\t-\4.\t.\4")
        buf.write("/\t/\3\2\3\2\3\3\5\3b\n\3\3\3\3\3\5\3f\n\3\3\3\5\3i\n")
        buf.write("\3\3\4\3\4\3\4\3\5\3\5\3\5\3\5\3\5\7\5s\n\5\f\5\16\5v")
        buf.write("\13\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\3\5\5\5\u0081\n")
        buf.write("\5\5\5\u0083\n\5\3\6\3\6\5\6\u0087\n\6\3\7\3\7\3\7\3\7")
        buf.write("\3\7\5\7\u008e\n\7\3\7\7\7\u0091\n\7\f\7\16\7\u0094\13")
        buf.write("\7\3\b\3\b\3\b\5\b\u0099\n\b\3\b\3\b\3\b\7\b\u009e\n\b")
        buf.write("\f\b\16\b\u00a1\13\b\3\t\3\t\3\t\3\t\3\t\5\t\u00a8\n\t")
        buf.write("\3\t\3\t\3\t\3\t\3\t\3\n\3\n\3\n\7\n\u00b2\n\n\f\n\16")
        buf.write("\n\u00b5\13\n\3\13\3\13\5\13\u00b9\n\13\3\13\3\13\3\13")
        buf.write("\5\13\u00be\n\13\3\13\3\13\5\13\u00c2\n\13\5\13\u00c4")
        buf.write("\n\13\3\13\3\13\3\13\5\13\u00c9\n\13\3\13\3\13\3\13\3")
        buf.write("\13\7\13\u00cf\n\13\f\13\16\13\u00d2\13\13\5\13\u00d4")
        buf.write("\n\13\3\13\3\13\5\13\u00d8\n\13\3\13\3\13\3\13\3\13\3")
        buf.write("\13\7\13\u00df\n\13\f\13\16\13\u00e2\13\13\5\13\u00e4")
        buf.write("\n\13\3\13\3\13\5\13\u00e8\n\13\3\f\3\f\5\f\u00ec\n\f")
        buf.write("\3\f\3\f\5\f\u00f0\n\f\3\f\3\f\3\f\3\f\3\f\6\f\u00f7\n")
        buf.write("\f\r\f\16\f\u00f8\5\f\u00fb\n\f\3\r\3\r\3\r\3\r\3\r\5")
        buf.write("\r\u0102\n\r\3\16\3\16\7\16\u0106\n\16\f\16\16\16\u0109")
        buf.write("\13\16\3\17\3\17\5\17\u010d\n\17\3\17\3\17\3\17\5\17\u0112")
        buf.write("\n\17\5\17\u0114\n\17\3\17\3\17\5\17\u0118\n\17\5\17\u011a")
        buf.write("\n\17\3\20\3\20\3\21\3\21\3\21\3\21\3\21\3\21\5\21\u0124")
        buf.write("\n\21\3\22\3\22\3\22\3\22\3\22\3\22\3\22\3\22\3\22\7\22")
        buf.write("\u012f\n\22\f\22\16\22\u0132\13\22\3\23\3\23\3\23\7\23")
        buf.write("\u0137\n\23\f\23\16\23\u013a\13\23\3\24\3\24\3\25\5\25")
        buf.write("\u013f\n\25\3\25\3\25\5\25\u0143\n\25\5\25\u0145\n\25")
        buf.write("\3\25\5\25\u0148\n\25\3\25\3\25\3\25\3\25\3\25\3\25\3")
        buf.write("\25\3\25\3\25\3\25\3\25\3\25\3\25\3\25\5\25\u0158\n\25")
        buf.write("\3\26\3\26\5\26\u015c\n\26\3\27\5\27\u015f\n\27\3\27\3")
        buf.write("\27\3\30\3\30\5\30\u0165\n\30\3\31\5\31\u0168\n\31\3\31")
        buf.write("\3\31\3\31\3\31\7\31\u016e\n\31\f\31\16\31\u0171\13\31")
        buf.write("\3\31\3\31\3\32\5\32\u0176\n\32\3\32\3\32\3\32\3\32\3")
        buf.write("\32\7\32\u017d\n\32\f\32\16\32\u0180\13\32\3\32\3\32\3")
        buf.write("\32\3\32\3\32\3\32\5\32\u0188\n\32\3\33\3\33\3\34\3\34")
        buf.write("\3\34\3\34\7\34\u0190\n\34\f\34\16\34\u0193\13\34\3\34")
        buf.write("\3\34\3\35\3\35\5\35\u0199\n\35\3\36\3\36\3\36\3\36\3")
        buf.write("\36\3\36\3\36\3\36\3\36\3\36\3\36\3\36\3\36\3\36\3\36")
        buf.write("\3\36\3\36\3\36\3\36\5\36\u01ae\n\36\3\36\3\36\3\36\3")
        buf.write("\36\3\36\3\36\3\36\3\36\3\36\3\36\3\36\3\36\3\36\7\36")
        buf.write("\u01bd\n\36\f\36\16\36\u01c0\13\36\3\37\3\37\3\37\3\37")
        buf.write("\5\37\u01c6\n\37\3\37\3\37\5\37\u01ca\n\37\3\37\3\37\5")
        buf.write("\37\u01ce\n\37\3 \3 \3!\5!\u01d3\n!\3!\3!\3\"\3\"\3\"")
        buf.write("\7\"\u01da\n\"\f\"\16\"\u01dd\13\"\3#\3#\3#\5#\u01e2\n")
        buf.write("#\3#\3#\3#\5#\u01e7\n#\3#\3#\3#\3#\3#\3#\5#\u01ef\n#\3")
        buf.write("#\5#\u01f2\n#\5#\u01f4\n#\3$\3$\3$\5$\u01f9\n$\3$\3$\3")
        buf.write("%\3%\3&\3&\3&\3&\3&\3&\3&\6&\u0206\n&\r&\16&\u0207\3&")
        buf.write("\3&\5&\u020c\n&\3&\3&\3&\3&\3&\3&\3&\3&\6&\u0216\n&\r")
        buf.write("&\16&\u0217\3&\3&\5&\u021c\n&\3&\3&\5&\u0220\n&\3\'\5")
        buf.write("\'\u0223\n\'\3\'\3\'\3\'\3\'\5\'\u0229\n\'\3\'\3\'\3(")
        buf.write("\3(\3(\7(\u0230\n(\f(\16(\u0233\13(\3)\3)\3)\7)\u0238")
        buf.write("\n)\f)\16)\u023b\13)\3*\5*\u023e\n*\3*\3*\3+\3+\3+\3+")
        buf.write("\3+\3+\3+\3+\3+\3+\3+\3+\3+\3+\3+\3+\3+\3+\5+\u0254\n")
        buf.write("+\3+\3+\3+\3+\3+\3+\3+\5+\u025d\n+\3+\3+\3+\3+\5+\u0263")
        buf.write("\n+\3+\3+\3+\3+\5+\u0269\n+\3+\3+\3+\3+\5+\u026f\n+\3")
        buf.write("+\3+\3+\3+\3+\3+\3+\3+\5+\u0279\n+\3,\3,\3,\5,\u027e\n")
        buf.write(",\3-\3-\3-\3-\3-\3-\3-\3-\3-\3-\3-\3-\3-\3-\3-\5-\u028f")
        buf.write("\n-\3.\3.\3/\5/\u0294\n/\3/\3/\3/\2\3:\60\2\4\6\b\n\f")
        buf.write("\16\20\22\24\26\30\32\34\36 \"$&(*,.\60\62\64\668:<>@")
        buf.write("BDFHJLNPRTVXZ\\\2\21\4\2\u0106\u0106\u0108\u0108\4\2\u00d3")
        buf.write("\u00d3\u00eb\u00eb\4\2\n\n\62\62\4\2\5\5\64\64\5\2KK`")
        buf.write("`\u008d\u008d\6\2dd\u00db\u00db\u00e6\u00e6\u0103\u0103")
        buf.write("\3\2\u012d\u012e\3\2\u014c\u014d\3\2\u0149\u014b\4\2\u014c")
        buf.write("\u014d\u014f\u0151\3\2\u0131\u0132\4\2\u012d\u012d\u0131")
        buf.write("\u0131\4\2RR\u0107\u0107\5\2\5\5\b\b\u009d\u009d\4\2\u00bd")
        buf.write("\u0125\u012e\u012e\u02e2\2^\3\2\2\2\4a\3\2\2\2\6j\3\2")
        buf.write("\2\2\bm\3\2\2\2\n\u0084\3\2\2\2\f\u008d\3\2\2\2\16\u0095")
        buf.write("\3\2\2\2\20\u00a2\3\2\2\2\22\u00ae\3\2\2\2\24\u00b6\3")
        buf.write("\2\2\2\26\u00ef\3\2\2\2\30\u0101\3\2\2\2\32\u0103\3\2")
        buf.write("\2\2\34\u0119\3\2\2\2\36\u011b\3\2\2\2 \u0123\3\2\2\2")
        buf.write("\"\u0125\3\2\2\2$\u0133\3\2\2\2&\u013b\3\2\2\2(\u0157")
        buf.write("\3\2\2\2*\u0159\3\2\2\2,\u015e\3\2\2\2.\u0162\3\2\2\2")
        buf.write("\60\u0167\3\2\2\2\62\u0175\3\2\2\2\64\u0189\3\2\2\2\66")
        buf.write("\u018b\3\2\2\28\u0198\3\2\2\2:\u01ad\3\2\2\2<\u01cd\3")
        buf.write("\2\2\2>\u01cf\3\2\2\2@\u01d2\3\2\2\2B\u01d6\3\2\2\2D\u01f3")
        buf.write("\3\2\2\2F\u01f8\3\2\2\2H\u01fc\3\2\2\2J\u021f\3\2\2\2")
        buf.write("L\u0228\3\2\2\2N\u022c\3\2\2\2P\u0234\3\2\2\2R\u023d\3")
        buf.write("\2\2\2T\u0278\3\2\2\2V\u027d\3\2\2\2X\u028e\3\2\2\2Z\u0290")
        buf.write("\3\2\2\2\\\u0293\3\2\2\2^_\5\4\3\2_\3\3\2\2\2`b\5\16\b")
        buf.write("\2a`\3\2\2\2ab\3\2\2\2bc\3\2\2\2ce\5\f\7\2df\5\b\5\2e")
        buf.write("d\3\2\2\2ef\3\2\2\2fh\3\2\2\2gi\5\6\4\2hg\3\2\2\2hi\3")
        buf.write("\2\2\2i\5\3\2\2\2jk\7\3\2\2kl\5@!\2l\7\3\2\2\2mn\7v\2")
        buf.write("\2no\7\22\2\2ot\5\n\6\2pq\7\u0146\2\2qs\5\n\6\2rp\3\2")
        buf.write("\2\2sv\3\2\2\2tr\3\2\2\2tu\3\2\2\2u\u0082\3\2\2\2vt\3")
        buf.write("\2\2\2wx\7\u00f2\2\2xy\5:\36\2y\u0080\t\2\2\2z{\7C\2\2")
        buf.write("{|\t\3\2\2|}\5:\36\2}~\t\2\2\2~\177\7\u00f3\2\2\177\u0081")
        buf.write("\3\2\2\2\u0080z\3\2\2\2\u0080\u0081\3\2\2\2\u0081\u0083")
        buf.write("\3\2\2\2\u0082w\3\2\2\2\u0082\u0083\3\2\2\2\u0083\t\3")
        buf.write("\2\2\2\u0084\u0086\5:\36\2\u0085\u0087\t\4\2\2\u0086\u0085")
        buf.write("\3\2\2\2\u0086\u0087\3\2\2\2\u0087\13\3\2\2\2\u0088\u008e")
        buf.write("\5\24\13\2\u0089\u008a\7\u0144\2\2\u008a\u008b\5\f\7\2")
        buf.write("\u008b\u008c\7\u0145\2\2\u008c\u008e\3\2\2\2\u008d\u0088")
        buf.write("\3\2\2\2\u008d\u0089\3\2\2\2\u008e\u0092\3\2\2\2\u008f")
        buf.write("\u0091\5\26\f\2\u0090\u008f\3\2\2\2\u0091\u0094\3\2\2")
        buf.write("\2\u0092\u0090\3\2\2\2\u0092\u0093\3\2\2\2\u0093\r\3\2")
        buf.write("\2\2\u0094\u0092\3\2\2\2\u0095\u0098\7\u00ba\2\2\u0096")
        buf.write("\u0097\7\u0125\2\2\u0097\u0099\7\u0146\2\2\u0098\u0096")
        buf.write("\3\2\2\2\u0098\u0099\3\2\2\2\u0099\u009a\3\2\2\2\u009a")
        buf.write("\u009f\5\20\t\2\u009b\u009c\7\u0146\2\2\u009c\u009e\5")
        buf.write("\20\t\2\u009d\u009b\3\2\2\2\u009e\u00a1\3\2\2\2\u009f")
        buf.write("\u009d\3\2\2\2\u009f\u00a0\3\2\2\2\u00a0\17\3\2\2\2\u00a1")
        buf.write("\u009f\3\2\2\2\u00a2\u00a7\5V,\2\u00a3\u00a4\7\u0144\2")
        buf.write("\2\u00a4\u00a5\5\22\n\2\u00a5\u00a6\7\u0145\2\2\u00a6")
        buf.write("\u00a8\3\2\2\2\u00a7\u00a3\3\2\2\2\u00a7\u00a8\3\2\2\2")
        buf.write("\u00a8\u00a9\3\2\2\2\u00a9\u00aa\7\t\2\2\u00aa\u00ab\7")
        buf.write("\u0144\2\2\u00ab\u00ac\5\4\3\2\u00ac\u00ad\7\u0145\2\2")
        buf.write("\u00ad\21\3\2\2\2\u00ae\u00b3\5H%\2\u00af\u00b0\7\u0146")
        buf.write("\2\2\u00b0\u00b2\5H%\2\u00b1\u00af\3\2\2\2\u00b2\u00b5")
        buf.write("\3\2\2\2\u00b3\u00b1\3\2\2\2\u00b3\u00b4\3\2\2\2\u00b4")
        buf.write("\23\3\2\2\2\u00b5\u00b3\3\2\2\2\u00b6\u00b8\7\u0095\2")
        buf.write("\2\u00b7\u00b9\t\5\2\2\u00b8\u00b7\3\2\2\2\u00b8\u00b9")
        buf.write("\3\2\2\2\u00b9\u00c3\3\2\2\2\u00ba\u00bb\7\u00a5\2\2\u00bb")
        buf.write("\u00bd\5:\36\2\u00bc\u00be\7y\2\2\u00bd\u00bc\3\2\2\2")
        buf.write("\u00bd\u00be\3\2\2\2\u00be\u00c1\3\2\2\2\u00bf\u00c0\7")
        buf.write("\u00ba\2\2\u00c0\u00c2\7\u0117\2\2\u00c1\u00bf\3\2\2\2")
        buf.write("\u00c1\u00c2\3\2\2\2\u00c2\u00c4\3\2\2\2\u00c3\u00ba\3")
        buf.write("\2\2\2\u00c3\u00c4\3\2\2\2\u00c4\u00c5\3\2\2\2\u00c5\u00c8")
        buf.write("\5B\"\2\u00c6\u00c7\7[\2\2\u00c7\u00c9\5L\'\2\u00c8\u00c6")
        buf.write("\3\2\2\2\u00c8\u00c9\3\2\2\2\u00c9\u00d3\3\2\2\2\u00ca")
        buf.write("\u00cb\7J\2\2\u00cb\u00d0\5\30\r\2\u00cc\u00cd\7\u0146")
        buf.write("\2\2\u00cd\u00cf\5\30\r\2\u00ce\u00cc\3\2\2\2\u00cf\u00d2")
        buf.write("\3\2\2\2\u00d0\u00ce\3\2\2\2\u00d0\u00d1\3\2\2\2\u00d1")
        buf.write("\u00d4\3\2\2\2\u00d2\u00d0\3\2\2\2\u00d3\u00ca\3\2\2\2")
        buf.write("\u00d3\u00d4\3\2\2\2\u00d4\u00d7\3\2\2\2\u00d5\u00d6\7")
        buf.write("\u00b8\2\2\u00d6\u00d8\5N(\2\u00d7\u00d5\3\2\2\2\u00d7")
        buf.write("\u00d8\3\2\2\2\u00d8\u00e3\3\2\2\2\u00d9\u00da\7O\2\2")
        buf.write("\u00da\u00db\7\22\2\2\u00db\u00e0\5\36\20\2\u00dc\u00dd")
        buf.write("\7\u0146\2\2\u00dd\u00df\5\36\20\2\u00de\u00dc\3\2\2\2")
        buf.write("\u00df\u00e2\3\2\2\2\u00e0\u00de\3\2\2\2\u00e0\u00e1\3")
        buf.write("\2\2\2\u00e1\u00e4\3\2\2\2\u00e2\u00e0\3\2\2\2\u00e3\u00d9")
        buf.write("\3\2\2\2\u00e3\u00e4\3\2\2\2\u00e4\u00e7\3\2\2\2\u00e5")
        buf.write("\u00e6\7P\2\2\u00e6\u00e8\5N(\2\u00e7\u00e5\3\2\2\2\u00e7")
        buf.write("\u00e8\3\2\2\2\u00e8\25\3\2\2\2\u00e9\u00eb\7\u00ac\2")
        buf.write("\2\u00ea\u00ec\7\5\2\2\u00eb\u00ea\3\2\2\2\u00eb\u00ec")
        buf.write("\3\2\2\2\u00ec\u00f0\3\2\2\2\u00ed\u00f0\7=\2\2\u00ee")
        buf.write("\u00f0\7Z\2\2\u00ef\u00e9\3\2\2\2\u00ef\u00ed\3\2\2\2")
        buf.write("\u00ef\u00ee\3\2\2\2\u00f0\u00fa\3\2\2\2\u00f1\u00fb\5")
        buf.write("\24\13\2\u00f2\u00f3\7\u0144\2\2\u00f3\u00f4\5\f\7\2\u00f4")
        buf.write("\u00f5\7\u0145\2\2\u00f5\u00f7\3\2\2\2\u00f6\u00f2\3\2")
        buf.write("\2\2\u00f7\u00f8\3\2\2\2\u00f8\u00f6\3\2\2\2\u00f8\u00f9")
        buf.write("\3\2\2\2\u00f9\u00fb\3\2\2\2\u00fa\u00f1\3\2\2\2\u00fa")
        buf.write("\u00f6\3\2\2\2\u00fb\27\3\2\2\2\u00fc\u0102\5\32\16\2")
        buf.write("\u00fd\u00fe\7\u0144\2\2\u00fe\u00ff\5\32\16\2\u00ff\u0100")
        buf.write("\7\u0145\2\2\u0100\u0102\3\2\2\2\u0101\u00fc\3\2\2\2\u0101")
        buf.write("\u00fd\3\2\2\2\u0102\31\3\2\2\2\u0103\u0107\5\34\17\2")
        buf.write("\u0104\u0106\5(\25\2\u0105\u0104\3\2\2\2\u0106\u0109\3")
        buf.write("\2\2\2\u0107\u0105\3\2\2\2\u0107\u0108\3\2\2\2\u0108\33")
        buf.write("\3\2\2\2\u0109\u0107\3\2\2\2\u010a\u010c\5*\26\2\u010b")
        buf.write("\u010d\5,\27\2\u010c\u010b\3\2\2\2\u010c\u010d\3\2\2\2")
        buf.write("\u010d\u011a\3\2\2\2\u010e\u0113\5 \21\2\u010f\u0111\5")
        buf.write(",\27\2\u0110\u0112\5\66\34\2\u0111\u0110\3\2\2\2\u0111")
        buf.write("\u0112\3\2\2\2\u0112\u0114\3\2\2\2\u0113\u010f\3\2\2\2")
        buf.write("\u0113\u0114\3\2\2\2\u0114\u011a\3\2\2\2\u0115\u0117\7")
        buf.write("\u012c\2\2\u0116\u0118\5,\27\2\u0117\u0116\3\2\2\2\u0117")
        buf.write("\u0118\3\2\2\2\u0118\u011a\3\2\2\2\u0119\u010a\3\2\2\2")
        buf.write("\u0119\u010e\3\2\2\2\u0119\u0115\3\2\2\2\u011a\35\3\2")
        buf.write("\2\2\u011b\u011c\5:\36\2\u011c\37\3\2\2\2\u011d\u0124")
        buf.write("\5&\24\2\u011e\u011f\7\u0144\2\2\u011f\u0120\5&\24\2\u0120")
        buf.write("\u0121\7\u0145\2\2\u0121\u0124\3\2\2\2\u0122\u0124\5\"")
        buf.write("\22\2\u0123\u011d\3\2\2\2\u0123\u011e\3\2\2\2\u0123\u0122")
        buf.write("\3\2\2\2\u0124!\3\2\2\2\u0125\u0126\7\u00b3\2\2\u0126")
        buf.write("\u0127\7\u0144\2\2\u0127\u0128\5$\23\2\u0128\u0130\7\u0145")
        buf.write("\2\2\u0129\u012a\7\u0146\2\2\u012a\u012b\7\u0144\2\2\u012b")
        buf.write("\u012c\5$\23\2\u012c\u012d\7\u0145\2\2\u012d\u012f\3\2")
        buf.write("\2\2\u012e\u0129\3\2\2\2\u012f\u0132\3\2\2\2\u0130\u012e")
        buf.write("\3\2\2\2\u0130\u0131\3\2\2\2\u0131#\3\2\2\2\u0132\u0130")
        buf.write("\3\2\2\2\u0133\u0138\5:\36\2\u0134\u0135\7\u0146\2\2\u0135")
        buf.write("\u0137\5:\36\2\u0136\u0134\3\2\2\2\u0137\u013a\3\2\2\2")
        buf.write("\u0138\u0136\3\2\2\2\u0138\u0139\3\2\2\2\u0139%\3\2\2")
        buf.write("\2\u013a\u0138\3\2\2\2\u013b\u013c\5\4\3\2\u013c\'\3\2")
        buf.write("\2\2\u013d\u013f\7X\2\2\u013e\u013d\3\2\2\2\u013e\u013f")
        buf.write("\3\2\2\2\u013f\u0145\3\2\2\2\u0140\u0142\t\6\2\2\u0141")
        buf.write("\u0143\7w\2\2\u0142\u0141\3\2\2\2\u0142\u0143\3\2\2\2")
        buf.write("\u0143\u0145\3\2\2\2\u0144\u013e\3\2\2\2\u0144\u0140\3")
        buf.write("\2\2\2\u0145\u0147\3\2\2\2\u0146\u0148\t\7\2\2\u0147\u0146")
        buf.write("\3\2\2\2\u0147\u0148\3\2\2\2\u0148\u0149\3\2\2\2\u0149")
        buf.write("\u014a\7]\2\2\u014a\u014b\5\30\r\2\u014b\u014c\7n\2\2")
        buf.write("\u014c\u014d\5N(\2\u014d\u0158\3\2\2\2\u014e\u014f\7$")
        buf.write("\2\2\u014f\u0150\7]\2\2\u0150\u0158\5\30\r\2\u0151\u0152")
        buf.write("\7$\2\2\u0152\u0153\7\u00be\2\2\u0153\u0158\5\30\r\2\u0154")
        buf.write("\u0155\7w\2\2\u0155\u0156\7\u00be\2\2\u0156\u0158\5\30")
        buf.write("\r\2\u0157\u0144\3\2\2\2\u0157\u014e\3\2\2\2\u0157\u0151")
        buf.write("\3\2\2\2\u0157\u0154\3\2\2\2\u0158)\3\2\2\2\u0159\u015b")
        buf.write("\5L\'\2\u015a\u015c\5\60\31\2\u015b\u015a\3\2\2\2\u015b")
        buf.write("\u015c\3\2\2\2\u015c+\3\2\2\2\u015d\u015f\7\t\2\2\u015e")
        buf.write("\u015d\3\2\2\2\u015e\u015f\3\2\2\2\u015f\u0160\3\2\2\2")
        buf.write("\u0160\u0161\5.\30\2\u0161-\3\2\2\2\u0162\u0164\5V,\2")
        buf.write("\u0163\u0165\5\60\31\2\u0164\u0163\3\2\2\2\u0164\u0165")
        buf.write("\3\2\2\2\u0165/\3\2\2\2\u0166\u0168\7\u00ba\2\2\u0167")
        buf.write("\u0166\3\2\2\2\u0167\u0168\3\2\2\2\u0168\u0169\3\2\2\2")
        buf.write("\u0169\u016a\7\u0144\2\2\u016a\u016f\5\62\32\2\u016b\u016c")
        buf.write("\7\u0146\2\2\u016c\u016e\5\62\32\2\u016d\u016b\3\2\2\2")
        buf.write("\u016e\u0171\3\2\2\2\u016f\u016d\3\2\2\2\u016f\u0170\3")
        buf.write("\2\2\2\u0170\u0172\3\2\2\2\u0171\u016f\3\2\2\2\u0172\u0173")
        buf.write("\7\u0145\2\2\u0173\61\3\2\2\2\u0174\u0176\7\u00ee\2\2")
        buf.write("\u0175\u0174\3\2\2\2\u0175\u0176\3\2\2\2\u0176\u0187\3")
        buf.write("\2\2\2\u0177\u0178\7W\2\2\u0178\u0179\7\u0144\2\2\u0179")
        buf.write("\u017e\5\64\33\2\u017a\u017b\7\u0146\2\2\u017b\u017d\5")
        buf.write("\64\33\2\u017c\u017a\3\2\2\2\u017d\u0180\3\2\2\2\u017e")
        buf.write("\u017c\3\2\2\2\u017e\u017f\3\2\2\2\u017f\u0181\3\2\2\2")
        buf.write("\u0180\u017e\3\2\2\2\u0181\u0182\7\u0145\2\2\u0182\u0188")
        buf.write("\3\2\2\2\u0183\u0184\7W\2\2\u0184\u0185\7\u0133\2\2\u0185")
        buf.write("\u0188\5\64\33\2\u0186\u0188\7\u012e\2\2\u0187\u0177\3")
        buf.write("\2\2\2\u0187\u0183\3\2\2\2\u0187\u0186\3\2\2\2\u0188\63")
        buf.write("\3\2\2\2\u0189\u018a\t\b\2\2\u018a\65\3\2\2\2\u018b\u018c")
        buf.write("\7\u0144\2\2\u018c\u0191\58\35\2\u018d\u018e\7\u0146\2")
        buf.write("\2\u018e\u0190\58\35\2\u018f\u018d\3\2\2\2\u0190\u0193")
        buf.write("\3\2\2\2\u0191\u018f\3\2\2\2\u0191\u0192\3\2\2\2\u0192")
        buf.write("\u0194\3\2\2\2\u0193\u0191\3\2\2\2\u0194\u0195\7\u0145")
        buf.write("\2\2\u0195\67\3\2\2\2\u0196\u0199\5V,\2\u0197\u0199\7")
        buf.write("\u012f\2\2\u0198\u0196\3\2\2\2\u0198\u0197\3\2\2\2\u0199")
        buf.write("9\3\2\2\2\u019a\u019b\b\36\1\2\u019b\u019c\7\u014e\2\2")
        buf.write("\u019c\u01ae\5:\36\7\u019d\u019e\t\t\2\2\u019e\u01ae\5")
        buf.write(":\36\5\u019f\u01ae\7/\2\2\u01a0\u01ae\7i\2\2\u01a1\u01ae")
        buf.write("\7\u012c\2\2\u01a2\u01ae\5<\37\2\u01a3\u01ae\5J&\2\u01a4")
        buf.write("\u01ae\5F$\2\u01a5\u01a6\7\u0144\2\2\u01a6\u01a7\5:\36")
        buf.write("\2\u01a7\u01a8\7\u0145\2\2\u01a8\u01ae\3\2\2\2\u01a9\u01aa")
        buf.write("\7\u0144\2\2\u01aa\u01ab\5&\24\2\u01ab\u01ac\7\u0145\2")
        buf.write("\2\u01ac\u01ae\3\2\2\2\u01ad\u019a\3\2\2\2\u01ad\u019d")
        buf.write("\3\2\2\2\u01ad\u019f\3\2\2\2\u01ad\u01a0\3\2\2\2\u01ad")
        buf.write("\u01a1\3\2\2\2\u01ad\u01a2\3\2\2\2\u01ad\u01a3\3\2\2\2")
        buf.write("\u01ad\u01a4\3\2\2\2\u01ad\u01a5\3\2\2\2\u01ad\u01a9\3")
        buf.write("\2\2\2\u01ae\u01be\3\2\2\2\u01af\u01b0\f\6\2\2\u01b0\u01b1")
        buf.write("\t\n\2\2\u01b1\u01bd\5:\36\7\u01b2\u01b3\f\4\2\2\u01b3")
        buf.write("\u01b4\t\13\2\2\u01b4\u01bd\5:\36\5\u01b5\u01b6\f\3\2")
        buf.write("\2\u01b6\u01b7\5X-\2\u01b7\u01b8\5:\36\4\u01b8\u01bd\3")
        buf.write("\2\2\2\u01b9\u01ba\f\f\2\2\u01ba\u01bb\7\32\2\2\u01bb")
        buf.write("\u01bd\5V,\2\u01bc\u01af\3\2\2\2\u01bc\u01b2\3\2\2\2\u01bc")
        buf.write("\u01b5\3\2\2\2\u01bc\u01b9\3\2\2\2\u01bd\u01c0\3\2\2\2")
        buf.write("\u01be\u01bc\3\2\2\2\u01be\u01bf\3\2\2\2\u01bf;\3\2\2")
        buf.write("\2\u01c0\u01be\3\2\2\2\u01c1\u01ce\7\u012f\2\2\u01c2\u01ce")
        buf.write("\7\u0130\2\2\u01c3\u01ce\5@!\2\u01c4\u01c6\5> \2\u01c5")
        buf.write("\u01c4\3\2\2\2\u01c5\u01c6\3\2\2\2\u01c6\u01c7\3\2\2\2")
        buf.write("\u01c7\u01ce\t\f\2\2\u01c8\u01ca\5> \2\u01c9\u01c8\3\2")
        buf.write("\2\2\u01c9\u01ca\3\2\2\2\u01ca\u01cb\3\2\2\2\u01cb\u01cc")
        buf.write("\7\u0143\2\2\u01cc\u01ce\t\r\2\2\u01cd\u01c1\3\2\2\2\u01cd")
        buf.write("\u01c2\3\2\2\2\u01cd\u01c3\3\2\2\2\u01cd\u01c5\3\2\2\2")
        buf.write("\u01cd\u01c9\3\2\2\2\u01ce=\3\2\2\2\u01cf\u01d0\t\t\2")
        buf.write("\2\u01d0?\3\2\2\2\u01d1\u01d3\5> \2\u01d2\u01d1\3\2\2")
        buf.write("\2\u01d2\u01d3\3\2\2\2\u01d3\u01d4\3\2\2\2\u01d4\u01d5")
        buf.write("\7\u012d\2\2\u01d5A\3\2\2\2\u01d6\u01db\5D#\2\u01d7\u01d8")
        buf.write("\7\u0146\2\2\u01d8\u01da\5D#\2\u01d9\u01d7\3\2\2\2\u01da")
        buf.write("\u01dd\3\2\2\2\u01db\u01d9\3\2\2\2\u01db\u01dc\3\2\2\2")
        buf.write("\u01dcC\3\2\2\2\u01dd\u01db\3\2\2\2\u01de\u01df\5L\'\2")
        buf.write("\u01df\u01e0\7\u013f\2\2\u01e0\u01e2\3\2\2\2\u01e1\u01de")
        buf.write("\3\2\2\2\u01e1\u01e2\3\2\2\2\u01e2\u01e6\3\2\2\2\u01e3")
        buf.write("\u01e7\7\u0149\2\2\u01e4\u01e5\7\u0143\2\2\u01e5\u01e7")
        buf.write("\t\16\2\2\u01e6\u01e3\3\2\2\2\u01e6\u01e4\3\2\2\2\u01e7")
        buf.write("\u01f4\3\2\2\2\u01e8\u01e9\58\35\2\u01e9\u01ea\7\u0133")
        buf.write("\2\2\u01ea\u01eb\5:\36\2\u01eb\u01f4\3\2\2\2\u01ec\u01f1")
        buf.write("\5:\36\2\u01ed\u01ef\7\t\2\2\u01ee\u01ed\3\2\2\2\u01ee")
        buf.write("\u01ef\3\2\2\2\u01ef\u01f0\3\2\2\2\u01f0\u01f2\58\35\2")
        buf.write("\u01f1\u01ee\3\2\2\2\u01f1\u01f2\3\2\2\2\u01f2\u01f4\3")
        buf.write("\2\2\2\u01f3\u01e1\3\2\2\2\u01f3\u01e8\3\2\2\2\u01f3\u01ec")
        buf.write("\3\2\2\2\u01f4E\3\2\2\2\u01f5\u01f6\5L\'\2\u01f6\u01f7")
        buf.write("\7\u013f\2\2\u01f7\u01f9\3\2\2\2\u01f8\u01f5\3\2\2\2\u01f8")
        buf.write("\u01f9\3\2\2\2\u01f9\u01fa\3\2\2\2\u01fa\u01fb\5H%\2\u01fb")
        buf.write("G\3\2\2\2\u01fc\u01fd\5V,\2\u01fdI\3\2\2\2\u01fe\u01ff")
        buf.write("\7\24\2\2\u01ff\u0205\5:\36\2\u0200\u0201\7\u00b7\2\2")
        buf.write("\u0201\u0202\5:\36\2\u0202\u0203\7\u00a3\2\2\u0203\u0204")
        buf.write("\5:\36\2\u0204\u0206\3\2\2\2\u0205\u0200\3\2\2\2\u0206")
        buf.write("\u0207\3\2\2\2\u0207\u0205\3\2\2\2\u0207\u0208\3\2\2\2")
        buf.write("\u0208\u020b\3\2\2\2\u0209\u020a\79\2\2\u020a\u020c\5")
        buf.write(":\36\2\u020b\u0209\3\2\2\2\u020b\u020c\3\2\2\2\u020c\u020d")
        buf.write("\3\2\2\2\u020d\u020e\7:\2\2\u020e\u0220\3\2\2\2\u020f")
        buf.write("\u0215\7\24\2\2\u0210\u0211\7\u00b7\2\2\u0211\u0212\5")
        buf.write("N(\2\u0212\u0213\7\u00a3\2\2\u0213\u0214\5:\36\2\u0214")
        buf.write("\u0216\3\2\2\2\u0215\u0210\3\2\2\2\u0216\u0217\3\2\2\2")
        buf.write("\u0217\u0215\3\2\2\2\u0217\u0218\3\2\2\2\u0218\u021b\3")
        buf.write("\2\2\2\u0219\u021a\79\2\2\u021a\u021c\5:\36\2\u021b\u0219")
        buf.write("\3\2\2\2\u021b\u021c\3\2\2\2\u021c\u021d\3\2\2\2\u021d")
        buf.write("\u021e\7:\2\2\u021e\u0220\3\2\2\2\u021f\u01fe\3\2\2\2")
        buf.write("\u021f\u020f\3\2\2\2\u0220K\3\2\2\2\u0221\u0223\5V,\2")
        buf.write("\u0222\u0221\3\2\2\2\u0222\u0223\3\2\2\2\u0223\u0224\3")
        buf.write("\2\2\2\u0224\u0229\7\u013f\2\2\u0225\u0226\5V,\2\u0226")
        buf.write("\u0227\7\u013f\2\2\u0227\u0229\3\2\2\2\u0228\u0222\3\2")
        buf.write("\2\2\u0228\u0225\3\2\2\2\u0228\u0229\3\2\2\2\u0229\u022a")
        buf.write("\3\2\2\2\u022a\u022b\5V,\2\u022bM\3\2\2\2\u022c\u0231")
        buf.write("\5P)\2\u022d\u022e\7\7\2\2\u022e\u0230\5P)\2\u022f\u022d")
        buf.write("\3\2\2\2\u0230\u0233\3\2\2\2\u0231\u022f\3\2\2\2\u0231")
        buf.write("\u0232\3\2\2\2\u0232O\3\2\2\2\u0233\u0231\3\2\2\2\u0234")
        buf.write("\u0239\5R*\2\u0235\u0236\7u\2\2\u0236\u0238\5R*\2\u0237")
        buf.write("\u0235\3\2\2\2\u0238\u023b\3\2\2\2\u0239\u0237\3\2\2\2")
        buf.write("\u0239\u023a\3\2\2\2\u023aQ\3\2\2\2\u023b\u0239\3\2\2")
        buf.write("\2\u023c\u023e\7h\2\2\u023d\u023c\3\2\2\2\u023d\u023e")
        buf.write("\3\2\2\2\u023e\u023f\3\2\2\2\u023f\u0240\5T+\2\u0240S")
        buf.write("\3\2\2\2\u0241\u0242\7@\2\2\u0242\u0243\7\u0144\2\2\u0243")
        buf.write("\u0244\5&\24\2\u0244\u0245\7\u0145\2\2\u0245\u0279\3\2")
        buf.write("\2\2\u0246\u0247\5:\36\2\u0247\u0248\5X-\2\u0248\u0249")
        buf.write("\5:\36\2\u0249\u0279\3\2\2\2\u024a\u024b\5:\36\2\u024b")
        buf.write("\u024c\5X-\2\u024c\u024d\t\17\2\2\u024d\u024e\7\u0144")
        buf.write("\2\2\u024e\u024f\5&\24\2\u024f\u0250\7\u0145\2\2\u0250")
        buf.write("\u0279\3\2\2\2\u0251\u0253\5:\36\2\u0252\u0254\7h\2\2")
        buf.write("\u0253\u0252\3\2\2\2\u0253\u0254\3\2\2\2\u0254\u0255\3")
        buf.write("\2\2\2\u0255\u0256\7\16\2\2\u0256\u0257\5:\36\2\u0257")
        buf.write("\u0258\7\7\2\2\u0258\u0259\5:\36\2\u0259\u0279\3\2\2\2")
        buf.write("\u025a\u025c\5:\36\2\u025b\u025d\7h\2\2\u025c\u025b\3")
        buf.write("\2\2\2\u025c\u025d\3\2\2\2\u025d\u025e\3\2\2\2\u025e\u025f")
        buf.write("\7V\2\2\u025f\u0262\7\u0144\2\2\u0260\u0263\5&\24\2\u0261")
        buf.write("\u0263\5$\23\2\u0262\u0260\3\2\2\2\u0262\u0261\3\2\2\2")
        buf.write("\u0263\u0264\3\2\2\2\u0264\u0265\7\u0145\2\2\u0265\u0279")
        buf.write("\3\2\2\2\u0266\u0268\5:\36\2\u0267\u0269\7h\2\2\u0268")
        buf.write("\u0267\3\2\2\2\u0268\u0269\3\2\2\2\u0269\u026a\3\2\2\2")
        buf.write("\u026a\u026b\7a\2\2\u026b\u026e\5:\36\2\u026c\u026d\7")
        buf.write("<\2\2\u026d\u026f\5:\36\2\u026e\u026c\3\2\2\2\u026e\u026f")
        buf.write("\3\2\2\2\u026f\u0279\3\2\2\2\u0270\u0271\5:\36\2\u0271")
        buf.write("\u0272\7\\\2\2\u0272\u0273\5\\/\2\u0273\u0279\3\2\2\2")
        buf.write("\u0274\u0275\7\u0144\2\2\u0275\u0276\5N(\2\u0276\u0277")
        buf.write("\7\u0145\2\2\u0277\u0279\3\2\2\2\u0278\u0241\3\2\2\2\u0278")
        buf.write("\u0246\3\2\2\2\u0278\u024a\3\2\2\2\u0278\u0251\3\2\2\2")
        buf.write("\u0278\u025a\3\2\2\2\u0278\u0266\3\2\2\2\u0278\u0270\3")
        buf.write("\2\2\2\u0278\u0274\3\2\2\2\u0279U\3\2\2\2\u027a\u027e")
        buf.write("\5Z.\2\u027b\u027e\7\u012a\2\2\u027c\u027e\7\u012b\2\2")
        buf.write("\u027d\u027a\3\2\2\2\u027d\u027b\3\2\2\2\u027d\u027c\3")
        buf.write("\2\2\2\u027eW\3\2\2\2\u027f\u028f\7\u0133\2\2\u0280\u028f")
        buf.write("\7\u0134\2\2\u0281\u028f\7\u0135\2\2\u0282\u0283\7\u0135")
        buf.write("\2\2\u0283\u028f\7\u0133\2\2\u0284\u0285\7\u0134\2\2\u0285")
        buf.write("\u028f\7\u0133\2\2\u0286\u0287\7\u0135\2\2\u0287\u028f")
        buf.write("\7\u0134\2\2\u0288\u0289\7\u0136\2\2\u0289\u028f\7\u0133")
        buf.write("\2\2\u028a\u028b\7\u0136\2\2\u028b\u028f\7\u0134\2\2\u028c")
        buf.write("\u028d\7\u0136\2\2\u028d\u028f\7\u0135\2\2\u028e\u027f")
        buf.write("\3\2\2\2\u028e\u0280\3\2\2\2\u028e\u0281\3\2\2\2\u028e")
        buf.write("\u0282\3\2\2\2\u028e\u0284\3\2\2\2\u028e\u0286\3\2\2\2")
        buf.write("\u028e\u0288\3\2\2\2\u028e\u028a\3\2\2\2\u028e\u028c\3")
        buf.write("\2\2\2\u028fY\3\2\2\2\u0290\u0291\t\20\2\2\u0291[\3\2")
        buf.write("\2\2\u0292\u0294\7h\2\2\u0293\u0292\3\2\2\2\u0293\u0294")
        buf.write("\3\2\2\2\u0294\u0295\3\2\2\2\u0295\u0296\7i\2\2\u0296")
        buf.write("]\3\2\2\2Xaeht\u0080\u0082\u0086\u008d\u0092\u0098\u009f")
        buf.write("\u00a7\u00b3\u00b8\u00bd\u00c1\u00c3\u00c8\u00d0\u00d3")
        buf.write("\u00d7\u00e0\u00e3\u00e7\u00eb\u00ef\u00f8\u00fa\u0101")
        buf.write("\u0107\u010c\u0111\u0113\u0117\u0119\u0123\u0130\u0138")
        buf.write("\u013e\u0142\u0144\u0147\u0157\u015b\u015e\u0164\u0167")
        buf.write("\u016f\u0175\u017e\u0187\u0191\u0198\u01ad\u01bc\u01be")
        buf.write("\u01c5\u01c9\u01cd\u01d2\u01db\u01e1\u01e6\u01ee\u01f1")
        buf.write("\u01f3\u01f8\u0207\u020b\u0217\u021b\u021f\u0222\u0228")
        buf.write("\u0231\u0239\u023d\u0253\u025c\u0262\u0268\u026e\u0278")
        buf.write("\u027d\u028e\u0293")
        return buf.getvalue()


class SparksqlParser ( Parser ):

    grammarFileName = "Sparksql.g4"

    atn = ATNDeserializer().deserialize(serializedATN())

    decisionsToDFA = [ DFA(ds, i) for i, ds in enumerate(atn.decisionToState) ]

    sharedContextCache = PredictionContextCache()

    literalNames = [ "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "<INVALID>", "<INVALID>", "<INVALID>", 
                     "<INVALID>", "'='", "'>'", "'<'", "'!'", "'+='", "'-='", 
                     "'*='", "'/='", "'%='", "'&='", "'^='", "'|='", "'.'", 
                     "'_'", "'@'", "'#'", "'$'", "'('", "')'", "','", "';'", 
                     "':'", "'*'", "'/'", "'%'", "'+'", "'-'", "'~'", "'|'", 
                     "'&'", "'^'" ]

    symbolicNames = [ "<INVALID>", "LIMIT", "ADD", "ALL", "ALTER", "AND", 
                      "ANY", "AS", "ASC", "AUTHORIZATION", "BACKUP", "BEGIN", 
                      "BETWEEN", "BREAK", "BROWSE", "BULK", "BY", "CASCADE", 
                      "CASE", "CHECK", "CHECKPOINT", "CLOSE", "CLUSTERED", 
                      "COALESCE", "COLLATE", "COLUMN", "COMMIT", "COMPUTE", 
                      "CONSTRAINT", "CONTAINS", "CONTAINSTABLE", "CONTINUE", 
                      "CONVERT", "CREATE", "CROSS", "CURRENT", "CURRENT_DATE", 
                      "CURRENT_TIME", "CURRENT_TIMESTAMP", "CURRENT_USER", 
                      "CURSOR", "DATABASE", "DBCC", "DEALLOCATE", "DECLARE", 
                      "DEFAULT", "DELETE", "DENY", "DESC", "DISK", "DISTINCT", 
                      "DISTRIBUTED", "DOUBLE", "DROP", "DUMP", "ELSE", "END", 
                      "ERRLVL", "ESCAPE", "EXCEPT", "EXEC", "EXECUTE", "EXISTS", 
                      "EXIT", "EXTERNAL", "FETCH", "FILE", "FILLFACTOR", 
                      "FOR", "FOREIGN", "FREETEXT", "FREETEXTTABLE", "FROM", 
                      "FULL", "FUNCTION", "GOTO", "GRANT", "GROUP", "HAVING", 
                      "HOLDLOCK", "IDENTITY", "IDENTITYCOL", "IDENTITY_INSERT", 
                      "IF", "IN", "INDEX", "INNER", "INSERT", "INTERSECT", 
                      "INTO", "IS", "JOIN", "KEY", "KILL", "LEFT", "LIKE", 
                      "LINENO", "LOAD", "MERGE", "NATIONAL", "NOCHECK", 
                      "NONCLUSTERED", "NOT", "NULL", "NULLIF", "OF", "OFF", 
                      "OFFSETS", "ON", "OPEN", "OPENDATASOURCE", "OPENQUERY", 
                      "OPENROWSET", "OPENXML", "OPTION", "OR", "ORDER", 
                      "OUTER", "OVER", "PERCENT", "PIVOT", "PLAN", "PRECISION", 
                      "PRIMARY", "PRINT", "PROC", "PROCEDURE", "PUBLIC", 
                      "RAISERROR", "READ", "READTEXT", "RECONFIGURE", "REFERENCES", 
                      "REPLICATION", "RESTORE", "RESTRICT", "RETURN", "REVERT", 
                      "REVOKE", "RIGHT", "ROLLBACK", "ROWCOUNT", "ROWGUIDCOL", 
                      "RULE", "SAVE", "SCHEMA", "SECURITYAUDIT", "SELECT", 
                      "SEMANTICKEYPHRASETABLE", "SEMANTICSIMILARITYDETAILSTABLE", 
                      "SEMANTICSIMILARITYTABLE", "SESSION_USER", "SET", 
                      "SETUSER", "SHUTDOWN", "SOME", "STATISTICS", "SYSTEM_USER", 
                      "TABLE", "TABLESAMPLE", "TEXTSIZE", "THEN", "TO", 
                      "TOP", "TRAN", "TRANSACTION", "TRIGGER", "TRUNCATE", 
                      "TRY_CONVERT", "TSEQUAL", "UNION", "UNIQUE", "UNPIVOT", 
                      "UPDATE", "UPDATETEXT", "USE", "USER", "VALUES", "VARYING", 
                      "VIEW", "WAITFOR", "WHEN", "WHERE", "WHILE", "WITH", 
                      "WITHIN", "WRITETEXT", "ABSOLUTE", "APPLY", "AUTO", 
                      "AVG", "BASE64", "CALLER", "CAST", "CATCH", "CHECKSUM_AGG", 
                      "COMMITTED", "CONCAT", "COOKIE", "COUNT", "COUNT_BIG", 
                      "DELAY", "DELETED", "DENSE_RANK", "DISABLE", "DYNAMIC", 
                      "ENCRYPTION", "FAST", "FAST_FORWARD", "FIRST", "FOLLOWING", 
                      "FORWARD_ONLY", "FULLSCAN", "GLOBAL", "GO", "GROUPING", 
                      "GROUPING_ID", "HASH", "INSENSITIVE", "INSERTED", 
                      "ISOLATION", "KEEPFIXED", "KEYSET", "LAST", "LEVEL", 
                      "LOCAL", "LOCK_ESCALATION", "LOGIN", "LOOP", "MARK", 
                      "MAX", "MIN", "MODIFY", "NEXT", "NAME", "NOCOUNT", 
                      "NOEXPAND", "NORECOMPUTE", "NTILE", "NUMBER", "OFFSET", 
                      "ONLY", "OPTIMISTIC", "OPTIMIZE", "OUT", "OUTPUT", 
                      "OWNER", "PARTITION", "PATH", "PRECEDING", "PRIOR", 
                      "RANGE", "RANK", "READONLY", "READ_ONLY", "RECOMPILE", 
                      "RELATIVE", "REMOTE", "REPEATABLE", "ROOT", "ROW", 
                      "ROWGUID", "ROWS", "ROW_NUMBER", "SAMPLE", "SCHEMABINDING", 
                      "SCROLL", "SCROLL_LOCKS", "SELF", "SERIALIZABLE", 
                      "SNAPSHOT", "STATIC", "STATS_STREAM", "STDEV", "STDEVP", 
                      "SUM", "THROW", "TIES", "TIME", "TRY", "TYPE", "TYPE_WARNING", 
                      "UNBOUNDED", "UNCOMMITTED", "UNKNOWN", "USING", "VAR", 
                      "VARP", "VIEW_METADATA", "WORK", "XML", "XMLNAMESPACES", 
                      "DOLLAR_ACTION", "SPACE", "COMMENT", "LINE_COMMENT", 
                      "DOUBLE_QUOTE_ID", "SQUARE_BRACKET_ID", "LOCAL_ID", 
                      "DECIMAL", "ID", "STRING", "BINARY", "FLOAT", "REAL", 
                      "EQUAL", "GREATER", "LESS", "EXCLAMATION", "PLUS_ASSIGN", 
                      "MINUS_ASSIGN", "MULT_ASSIGN", "DIV_ASSIGN", "MOD_ASSIGN", 
                      "AND_ASSIGN", "XOR_ASSIGN", "OR_ASSIGN", "DOT", "UNDERLINE", 
                      "AT", "SHARP", "DOLLAR", "LR_BRACKET", "RR_BRACKET", 
                      "COMMA", "SEMI", "COLON", "STAR", "DIVIDE", "MODULE", 
                      "PLUS", "MINUS", "BIT_NOT", "BIT_OR", "BIT_AND", "BIT_XOR" ]

    RULE_root = 0
    RULE_select_statement = 1
    RULE_limit_clause = 2
    RULE_order_by_clause = 3
    RULE_order_by_expression = 4
    RULE_query_expression = 5
    RULE_with_expression = 6
    RULE_common_table_expression = 7
    RULE_column_name_list = 8
    RULE_query_specification = 9
    RULE_union = 10
    RULE_table_source = 11
    RULE_table_source_item_joined = 12
    RULE_table_source_item = 13
    RULE_group_by_item = 14
    RULE_derived_table = 15
    RULE_table_value_constructor = 16
    RULE_expression_list = 17
    RULE_subquery = 18
    RULE_join_part = 19
    RULE_table_name_with_hint = 20
    RULE_as_table_alias = 21
    RULE_table_alias = 22
    RULE_with_table_hints = 23
    RULE_table_hint = 24
    RULE_index_value = 25
    RULE_column_alias_list = 26
    RULE_column_alias = 27
    RULE_expression = 28
    RULE_constant = 29
    RULE_sign = 30
    RULE_number = 31
    RULE_select_list = 32
    RULE_select_list_elem = 33
    RULE_full_column_name = 34
    RULE_column_name = 35
    RULE_case_expr = 36
    RULE_table_name = 37
    RULE_search_condition = 38
    RULE_search_condition_or = 39
    RULE_search_condition_not = 40
    RULE_predicate = 41
    RULE_id_1 = 42
    RULE_comparison_operator = 43
    RULE_simple_id = 44
    RULE_null_notnull = 45

    ruleNames =  [ "root", "select_statement", "limit_clause", "order_by_clause", 
                   "order_by_expression", "query_expression", "with_expression", 
                   "common_table_expression", "column_name_list", "query_specification", 
                   "union", "table_source", "table_source_item_joined", 
                   "table_source_item", "group_by_item", "derived_table", 
                   "table_value_constructor", "expression_list", "subquery", 
                   "join_part", "table_name_with_hint", "as_table_alias", 
                   "table_alias", "with_table_hints", "table_hint", "index_value", 
                   "column_alias_list", "column_alias", "expression", "constant", 
                   "sign", "number", "select_list", "select_list_elem", 
                   "full_column_name", "column_name", "case_expr", "table_name", 
                   "search_condition", "search_condition_or", "search_condition_not", 
                   "predicate", "id_1", "comparison_operator", "simple_id", 
                   "null_notnull" ]

    EOF = Token.EOF
    LIMIT=1
    ADD=2
    ALL=3
    ALTER=4
    AND=5
    ANY=6
    AS=7
    ASC=8
    AUTHORIZATION=9
    BACKUP=10
    BEGIN=11
    BETWEEN=12
    BREAK=13
    BROWSE=14
    BULK=15
    BY=16
    CASCADE=17
    CASE=18
    CHECK=19
    CHECKPOINT=20
    CLOSE=21
    CLUSTERED=22
    COALESCE=23
    COLLATE=24
    COLUMN=25
    COMMIT=26
    COMPUTE=27
    CONSTRAINT=28
    CONTAINS=29
    CONTAINSTABLE=30
    CONTINUE=31
    CONVERT=32
    CREATE=33
    CROSS=34
    CURRENT=35
    CURRENT_DATE=36
    CURRENT_TIME=37
    CURRENT_TIMESTAMP=38
    CURRENT_USER=39
    CURSOR=40
    DATABASE=41
    DBCC=42
    DEALLOCATE=43
    DECLARE=44
    DEFAULT=45
    DELETE=46
    DENY=47
    DESC=48
    DISK=49
    DISTINCT=50
    DISTRIBUTED=51
    DOUBLE=52
    DROP=53
    DUMP=54
    ELSE=55
    END=56
    ERRLVL=57
    ESCAPE=58
    EXCEPT=59
    EXEC=60
    EXECUTE=61
    EXISTS=62
    EXIT=63
    EXTERNAL=64
    FETCH=65
    FILE=66
    FILLFACTOR=67
    FOR=68
    FOREIGN=69
    FREETEXT=70
    FREETEXTTABLE=71
    FROM=72
    FULL=73
    FUNCTION=74
    GOTO=75
    GRANT=76
    GROUP=77
    HAVING=78
    HOLDLOCK=79
    IDENTITY=80
    IDENTITYCOL=81
    IDENTITY_INSERT=82
    IF=83
    IN=84
    INDEX=85
    INNER=86
    INSERT=87
    INTERSECT=88
    INTO=89
    IS=90
    JOIN=91
    KEY=92
    KILL=93
    LEFT=94
    LIKE=95
    LINENO=96
    LOAD=97
    MERGE=98
    NATIONAL=99
    NOCHECK=100
    NONCLUSTERED=101
    NOT=102
    NULL=103
    NULLIF=104
    OF=105
    OFF=106
    OFFSETS=107
    ON=108
    OPEN=109
    OPENDATASOURCE=110
    OPENQUERY=111
    OPENROWSET=112
    OPENXML=113
    OPTION=114
    OR=115
    ORDER=116
    OUTER=117
    OVER=118
    PERCENT=119
    PIVOT=120
    PLAN=121
    PRECISION=122
    PRIMARY=123
    PRINT=124
    PROC=125
    PROCEDURE=126
    PUBLIC=127
    RAISERROR=128
    READ=129
    READTEXT=130
    RECONFIGURE=131
    REFERENCES=132
    REPLICATION=133
    RESTORE=134
    RESTRICT=135
    RETURN=136
    REVERT=137
    REVOKE=138
    RIGHT=139
    ROLLBACK=140
    ROWCOUNT=141
    ROWGUIDCOL=142
    RULE=143
    SAVE=144
    SCHEMA=145
    SECURITYAUDIT=146
    SELECT=147
    SEMANTICKEYPHRASETABLE=148
    SEMANTICSIMILARITYDETAILSTABLE=149
    SEMANTICSIMILARITYTABLE=150
    SESSION_USER=151
    SET=152
    SETUSER=153
    SHUTDOWN=154
    SOME=155
    STATISTICS=156
    SYSTEM_USER=157
    TABLE=158
    TABLESAMPLE=159
    TEXTSIZE=160
    THEN=161
    TO=162
    TOP=163
    TRAN=164
    TRANSACTION=165
    TRIGGER=166
    TRUNCATE=167
    TRY_CONVERT=168
    TSEQUAL=169
    UNION=170
    UNIQUE=171
    UNPIVOT=172
    UPDATE=173
    UPDATETEXT=174
    USE=175
    USER=176
    VALUES=177
    VARYING=178
    VIEW=179
    WAITFOR=180
    WHEN=181
    WHERE=182
    WHILE=183
    WITH=184
    WITHIN=185
    WRITETEXT=186
    ABSOLUTE=187
    APPLY=188
    AUTO=189
    AVG=190
    BASE64=191
    CALLER=192
    CAST=193
    CATCH=194
    CHECKSUM_AGG=195
    COMMITTED=196
    CONCAT=197
    COOKIE=198
    COUNT=199
    COUNT_BIG=200
    DELAY=201
    DELETED=202
    DENSE_RANK=203
    DISABLE=204
    DYNAMIC=205
    ENCRYPTION=206
    FAST=207
    FAST_FORWARD=208
    FIRST=209
    FOLLOWING=210
    FORWARD_ONLY=211
    FULLSCAN=212
    GLOBAL=213
    GO=214
    GROUPING=215
    GROUPING_ID=216
    HASH=217
    INSENSITIVE=218
    INSERTED=219
    ISOLATION=220
    KEEPFIXED=221
    KEYSET=222
    LAST=223
    LEVEL=224
    LOCAL=225
    LOCK_ESCALATION=226
    LOGIN=227
    LOOP=228
    MARK=229
    MAX=230
    MIN=231
    MODIFY=232
    NEXT=233
    NAME=234
    NOCOUNT=235
    NOEXPAND=236
    NORECOMPUTE=237
    NTILE=238
    NUMBER=239
    OFFSET=240
    ONLY=241
    OPTIMISTIC=242
    OPTIMIZE=243
    OUT=244
    OUTPUT=245
    OWNER=246
    PARTITION=247
    PATH=248
    PRECEDING=249
    PRIOR=250
    RANGE=251
    RANK=252
    READONLY=253
    READ_ONLY=254
    RECOMPILE=255
    RELATIVE=256
    REMOTE=257
    REPEATABLE=258
    ROOT=259
    ROW=260
    ROWGUID=261
    ROWS=262
    ROW_NUMBER=263
    SAMPLE=264
    SCHEMABINDING=265
    SCROLL=266
    SCROLL_LOCKS=267
    SELF=268
    SERIALIZABLE=269
    SNAPSHOT=270
    STATIC=271
    STATS_STREAM=272
    STDEV=273
    STDEVP=274
    SUM=275
    THROW=276
    TIES=277
    TIME=278
    TRY=279
    TYPE=280
    TYPE_WARNING=281
    UNBOUNDED=282
    UNCOMMITTED=283
    UNKNOWN=284
    USING=285
    VAR=286
    VARP=287
    VIEW_METADATA=288
    WORK=289
    XML=290
    XMLNAMESPACES=291
    DOLLAR_ACTION=292
    SPACE=293
    COMMENT=294
    LINE_COMMENT=295
    DOUBLE_QUOTE_ID=296
    SQUARE_BRACKET_ID=297
    LOCAL_ID=298
    DECIMAL=299
    ID=300
    STRING=301
    BINARY=302
    FLOAT=303
    REAL=304
    EQUAL=305
    GREATER=306
    LESS=307
    EXCLAMATION=308
    PLUS_ASSIGN=309
    MINUS_ASSIGN=310
    MULT_ASSIGN=311
    DIV_ASSIGN=312
    MOD_ASSIGN=313
    AND_ASSIGN=314
    XOR_ASSIGN=315
    OR_ASSIGN=316
    DOT=317
    UNDERLINE=318
    AT=319
    SHARP=320
    DOLLAR=321
    LR_BRACKET=322
    RR_BRACKET=323
    COMMA=324
    SEMI=325
    COLON=326
    STAR=327
    DIVIDE=328
    MODULE=329
    PLUS=330
    MINUS=331
    BIT_NOT=332
    BIT_OR=333
    BIT_AND=334
    BIT_XOR=335

    def __init__(self, input:TokenStream):
        super().__init__(input)
        self.checkVersion("4.5.1")
        self._interp = ParserATNSimulator(self, self.atn, self.decisionsToDFA, self.sharedContextCache)
        self._predicates = None



    class RootContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def select_statement(self):
            return self.getTypedRuleContext(SparksqlParser.Select_statementContext,0)


        def getRuleIndex(self):
            return SparksqlParser.RULE_root

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterRoot" ):
                listener.enterRoot(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitRoot" ):
                listener.exitRoot(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitRoot" ):
                return visitor.visitRoot(self)
            else:
                return visitor.visitChildren(self)




    def root(self):

        localctx = SparksqlParser.RootContext(self, self._ctx, self.state)
        self.enterRule(localctx, 0, self.RULE_root)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 92
            self.select_statement()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Select_statementContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def query_expression(self):
            return self.getTypedRuleContext(SparksqlParser.Query_expressionContext,0)


        def with_expression(self):
            return self.getTypedRuleContext(SparksqlParser.With_expressionContext,0)


        def order_by_clause(self):
            return self.getTypedRuleContext(SparksqlParser.Order_by_clauseContext,0)


        def limit_clause(self):
            return self.getTypedRuleContext(SparksqlParser.Limit_clauseContext,0)


        def getRuleIndex(self):
            return SparksqlParser.RULE_select_statement

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSelect_statement" ):
                listener.enterSelect_statement(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSelect_statement" ):
                listener.exitSelect_statement(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSelect_statement" ):
                return visitor.visitSelect_statement(self)
            else:
                return visitor.visitChildren(self)




    def select_statement(self):

        localctx = SparksqlParser.Select_statementContext(self, self._ctx, self.state)
        self.enterRule(localctx, 2, self.RULE_select_statement)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 95
            _la = self._input.LA(1)
            if _la==SparksqlParser.WITH:
                self.state = 94
                self.with_expression()


            self.state = 97
            self.query_expression()
            self.state = 99
            la_ = self._interp.adaptivePredict(self._input,1,self._ctx)
            if la_ == 1:
                self.state = 98
                self.order_by_clause()


            self.state = 102
            la_ = self._interp.adaptivePredict(self._input,2,self._ctx)
            if la_ == 1:
                self.state = 101
                self.limit_clause()


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Limit_clauseContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def LIMIT(self):
            return self.getToken(SparksqlParser.LIMIT, 0)

        def number(self):
            return self.getTypedRuleContext(SparksqlParser.NumberContext,0)


        def getRuleIndex(self):
            return SparksqlParser.RULE_limit_clause

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterLimit_clause" ):
                listener.enterLimit_clause(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitLimit_clause" ):
                listener.exitLimit_clause(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitLimit_clause" ):
                return visitor.visitLimit_clause(self)
            else:
                return visitor.visitChildren(self)




    def limit_clause(self):

        localctx = SparksqlParser.Limit_clauseContext(self, self._ctx, self.state)
        self.enterRule(localctx, 4, self.RULE_limit_clause)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 104
            self.match(SparksqlParser.LIMIT)
            self.state = 105
            self.number()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Order_by_clauseContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def ORDER(self):
            return self.getToken(SparksqlParser.ORDER, 0)

        def BY(self):
            return self.getToken(SparksqlParser.BY, 0)

        def order_by_expression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.Order_by_expressionContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.Order_by_expressionContext,i)


        def OFFSET(self):
            return self.getToken(SparksqlParser.OFFSET, 0)

        def expression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.ExpressionContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.ExpressionContext,i)


        def ROW(self, i:int=None):
            if i is None:
                return self.getTokens(SparksqlParser.ROW)
            else:
                return self.getToken(SparksqlParser.ROW, i)

        def ROWS(self, i:int=None):
            if i is None:
                return self.getTokens(SparksqlParser.ROWS)
            else:
                return self.getToken(SparksqlParser.ROWS, i)

        def FETCH(self):
            return self.getToken(SparksqlParser.FETCH, 0)

        def ONLY(self):
            return self.getToken(SparksqlParser.ONLY, 0)

        def FIRST(self):
            return self.getToken(SparksqlParser.FIRST, 0)

        def NEXT(self):
            return self.getToken(SparksqlParser.NEXT, 0)

        def getRuleIndex(self):
            return SparksqlParser.RULE_order_by_clause

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterOrder_by_clause" ):
                listener.enterOrder_by_clause(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitOrder_by_clause" ):
                listener.exitOrder_by_clause(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitOrder_by_clause" ):
                return visitor.visitOrder_by_clause(self)
            else:
                return visitor.visitChildren(self)




    def order_by_clause(self):

        localctx = SparksqlParser.Order_by_clauseContext(self, self._ctx, self.state)
        self.enterRule(localctx, 6, self.RULE_order_by_clause)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 107
            self.match(SparksqlParser.ORDER)
            self.state = 108
            self.match(SparksqlParser.BY)
            self.state = 109
            self.order_by_expression()
            self.state = 114
            self._errHandler.sync(self)
            _alt = self._interp.adaptivePredict(self._input,3,self._ctx)
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt==1:
                    self.state = 110
                    self.match(SparksqlParser.COMMA)
                    self.state = 111
                    self.order_by_expression() 
                self.state = 116
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,3,self._ctx)

            self.state = 128
            la_ = self._interp.adaptivePredict(self._input,5,self._ctx)
            if la_ == 1:
                self.state = 117
                self.match(SparksqlParser.OFFSET)
                self.state = 118
                self.expression(0)
                self.state = 119
                _la = self._input.LA(1)
                if not(_la==SparksqlParser.ROW or _la==SparksqlParser.ROWS):
                    self._errHandler.recoverInline(self)
                else:
                    self.consume()
                self.state = 126
                _la = self._input.LA(1)
                if _la==SparksqlParser.FETCH:
                    self.state = 120
                    self.match(SparksqlParser.FETCH)
                    self.state = 121
                    _la = self._input.LA(1)
                    if not(_la==SparksqlParser.FIRST or _la==SparksqlParser.NEXT):
                        self._errHandler.recoverInline(self)
                    else:
                        self.consume()
                    self.state = 122
                    self.expression(0)
                    self.state = 123
                    _la = self._input.LA(1)
                    if not(_la==SparksqlParser.ROW or _la==SparksqlParser.ROWS):
                        self._errHandler.recoverInline(self)
                    else:
                        self.consume()
                    self.state = 124
                    self.match(SparksqlParser.ONLY)




        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Order_by_expressionContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def expression(self):
            return self.getTypedRuleContext(SparksqlParser.ExpressionContext,0)


        def ASC(self):
            return self.getToken(SparksqlParser.ASC, 0)

        def DESC(self):
            return self.getToken(SparksqlParser.DESC, 0)

        def getRuleIndex(self):
            return SparksqlParser.RULE_order_by_expression

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterOrder_by_expression" ):
                listener.enterOrder_by_expression(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitOrder_by_expression" ):
                listener.exitOrder_by_expression(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitOrder_by_expression" ):
                return visitor.visitOrder_by_expression(self)
            else:
                return visitor.visitChildren(self)




    def order_by_expression(self):

        localctx = SparksqlParser.Order_by_expressionContext(self, self._ctx, self.state)
        self.enterRule(localctx, 8, self.RULE_order_by_expression)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 130
            self.expression(0)
            self.state = 132
            _la = self._input.LA(1)
            if _la==SparksqlParser.ASC or _la==SparksqlParser.DESC:
                self.state = 131
                _la = self._input.LA(1)
                if not(_la==SparksqlParser.ASC or _la==SparksqlParser.DESC):
                    self._errHandler.recoverInline(self)
                else:
                    self.consume()


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Query_expressionContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def query_specification(self):
            return self.getTypedRuleContext(SparksqlParser.Query_specificationContext,0)


        def query_expression(self):
            return self.getTypedRuleContext(SparksqlParser.Query_expressionContext,0)


        def union(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.UnionContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.UnionContext,i)


        def getRuleIndex(self):
            return SparksqlParser.RULE_query_expression

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterQuery_expression" ):
                listener.enterQuery_expression(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitQuery_expression" ):
                listener.exitQuery_expression(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitQuery_expression" ):
                return visitor.visitQuery_expression(self)
            else:
                return visitor.visitChildren(self)




    def query_expression(self):

        localctx = SparksqlParser.Query_expressionContext(self, self._ctx, self.state)
        self.enterRule(localctx, 10, self.RULE_query_expression)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 139
            token = self._input.LA(1)
            if token in [SparksqlParser.SELECT]:
                self.state = 134
                self.query_specification()

            elif token in [SparksqlParser.LR_BRACKET]:
                self.state = 135
                self.match(SparksqlParser.LR_BRACKET)
                self.state = 136
                self.query_expression()
                self.state = 137
                self.match(SparksqlParser.RR_BRACKET)

            else:
                raise NoViableAltException(self)

            self.state = 144
            self._errHandler.sync(self)
            _alt = self._interp.adaptivePredict(self._input,8,self._ctx)
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt==1:
                    self.state = 141
                    self.union() 
                self.state = 146
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,8,self._ctx)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class With_expressionContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def WITH(self):
            return self.getToken(SparksqlParser.WITH, 0)

        def common_table_expression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.Common_table_expressionContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.Common_table_expressionContext,i)


        def XMLNAMESPACES(self):
            return self.getToken(SparksqlParser.XMLNAMESPACES, 0)

        def getRuleIndex(self):
            return SparksqlParser.RULE_with_expression

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterWith_expression" ):
                listener.enterWith_expression(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitWith_expression" ):
                listener.exitWith_expression(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitWith_expression" ):
                return visitor.visitWith_expression(self)
            else:
                return visitor.visitChildren(self)




    def with_expression(self):

        localctx = SparksqlParser.With_expressionContext(self, self._ctx, self.state)
        self.enterRule(localctx, 12, self.RULE_with_expression)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 147
            self.match(SparksqlParser.WITH)
            self.state = 150
            la_ = self._interp.adaptivePredict(self._input,9,self._ctx)
            if la_ == 1:
                self.state = 148
                self.match(SparksqlParser.XMLNAMESPACES)
                self.state = 149
                self.match(SparksqlParser.COMMA)


            self.state = 152
            self.common_table_expression()
            self.state = 157
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparksqlParser.COMMA:
                self.state = 153
                self.match(SparksqlParser.COMMA)
                self.state = 154
                self.common_table_expression()
                self.state = 159
                self._errHandler.sync(self)
                _la = self._input.LA(1)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Common_table_expressionContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.expression_name = None # Id_1Context

        def AS(self):
            return self.getToken(SparksqlParser.AS, 0)

        def select_statement(self):
            return self.getTypedRuleContext(SparksqlParser.Select_statementContext,0)


        def id_1(self):
            return self.getTypedRuleContext(SparksqlParser.Id_1Context,0)


        def column_name_list(self):
            return self.getTypedRuleContext(SparksqlParser.Column_name_listContext,0)


        def getRuleIndex(self):
            return SparksqlParser.RULE_common_table_expression

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterCommon_table_expression" ):
                listener.enterCommon_table_expression(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitCommon_table_expression" ):
                listener.exitCommon_table_expression(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitCommon_table_expression" ):
                return visitor.visitCommon_table_expression(self)
            else:
                return visitor.visitChildren(self)




    def common_table_expression(self):

        localctx = SparksqlParser.Common_table_expressionContext(self, self._ctx, self.state)
        self.enterRule(localctx, 14, self.RULE_common_table_expression)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 160
            localctx.expression_name = self.id_1()
            self.state = 165
            _la = self._input.LA(1)
            if _la==SparksqlParser.LR_BRACKET:
                self.state = 161
                self.match(SparksqlParser.LR_BRACKET)
                self.state = 162
                self.column_name_list()
                self.state = 163
                self.match(SparksqlParser.RR_BRACKET)


            self.state = 167
            self.match(SparksqlParser.AS)
            self.state = 168
            self.match(SparksqlParser.LR_BRACKET)
            self.state = 169
            self.select_statement()
            self.state = 170
            self.match(SparksqlParser.RR_BRACKET)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Column_name_listContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def column_name(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.Column_nameContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.Column_nameContext,i)


        def getRuleIndex(self):
            return SparksqlParser.RULE_column_name_list

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterColumn_name_list" ):
                listener.enterColumn_name_list(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitColumn_name_list" ):
                listener.exitColumn_name_list(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitColumn_name_list" ):
                return visitor.visitColumn_name_list(self)
            else:
                return visitor.visitChildren(self)




    def column_name_list(self):

        localctx = SparksqlParser.Column_name_listContext(self, self._ctx, self.state)
        self.enterRule(localctx, 16, self.RULE_column_name_list)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 172
            self.column_name()
            self.state = 177
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparksqlParser.COMMA:
                self.state = 173
                self.match(SparksqlParser.COMMA)
                self.state = 174
                self.column_name()
                self.state = 179
                self._errHandler.sync(self)
                _la = self._input.LA(1)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Query_specificationContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.into_table = None # Table_nameContext
            self.where = None # Search_conditionContext
            self.having = None # Search_conditionContext

        def SELECT(self):
            return self.getToken(SparksqlParser.SELECT, 0)

        def select_list(self):
            return self.getTypedRuleContext(SparksqlParser.Select_listContext,0)


        def TOP(self):
            return self.getToken(SparksqlParser.TOP, 0)

        def expression(self):
            return self.getTypedRuleContext(SparksqlParser.ExpressionContext,0)


        def INTO(self):
            return self.getToken(SparksqlParser.INTO, 0)

        def FROM(self):
            return self.getToken(SparksqlParser.FROM, 0)

        def table_source(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.Table_sourceContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.Table_sourceContext,i)


        def WHERE(self):
            return self.getToken(SparksqlParser.WHERE, 0)

        def GROUP(self):
            return self.getToken(SparksqlParser.GROUP, 0)

        def BY(self):
            return self.getToken(SparksqlParser.BY, 0)

        def group_by_item(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.Group_by_itemContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.Group_by_itemContext,i)


        def HAVING(self):
            return self.getToken(SparksqlParser.HAVING, 0)

        def ALL(self):
            return self.getToken(SparksqlParser.ALL, 0)

        def DISTINCT(self):
            return self.getToken(SparksqlParser.DISTINCT, 0)

        def table_name(self):
            return self.getTypedRuleContext(SparksqlParser.Table_nameContext,0)


        def search_condition(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.Search_conditionContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.Search_conditionContext,i)


        def PERCENT(self):
            return self.getToken(SparksqlParser.PERCENT, 0)

        def WITH(self):
            return self.getToken(SparksqlParser.WITH, 0)

        def TIES(self):
            return self.getToken(SparksqlParser.TIES, 0)

        def getRuleIndex(self):
            return SparksqlParser.RULE_query_specification

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterQuery_specification" ):
                listener.enterQuery_specification(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitQuery_specification" ):
                listener.exitQuery_specification(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitQuery_specification" ):
                return visitor.visitQuery_specification(self)
            else:
                return visitor.visitChildren(self)




    def query_specification(self):

        localctx = SparksqlParser.Query_specificationContext(self, self._ctx, self.state)
        self.enterRule(localctx, 18, self.RULE_query_specification)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 180
            self.match(SparksqlParser.SELECT)
            self.state = 182
            _la = self._input.LA(1)
            if _la==SparksqlParser.ALL or _la==SparksqlParser.DISTINCT:
                self.state = 181
                _la = self._input.LA(1)
                if not(_la==SparksqlParser.ALL or _la==SparksqlParser.DISTINCT):
                    self._errHandler.recoverInline(self)
                else:
                    self.consume()


            self.state = 193
            _la = self._input.LA(1)
            if _la==SparksqlParser.TOP:
                self.state = 184
                self.match(SparksqlParser.TOP)
                self.state = 185
                self.expression(0)
                self.state = 187
                _la = self._input.LA(1)
                if _la==SparksqlParser.PERCENT:
                    self.state = 186
                    self.match(SparksqlParser.PERCENT)


                self.state = 191
                _la = self._input.LA(1)
                if _la==SparksqlParser.WITH:
                    self.state = 189
                    self.match(SparksqlParser.WITH)
                    self.state = 190
                    self.match(SparksqlParser.TIES)




            self.state = 195
            self.select_list()
            self.state = 198
            _la = self._input.LA(1)
            if _la==SparksqlParser.INTO:
                self.state = 196
                self.match(SparksqlParser.INTO)
                self.state = 197
                localctx.into_table = self.table_name()


            self.state = 209
            _la = self._input.LA(1)
            if _la==SparksqlParser.FROM:
                self.state = 200
                self.match(SparksqlParser.FROM)
                self.state = 201
                self.table_source()
                self.state = 206
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,18,self._ctx)
                while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                    if _alt==1:
                        self.state = 202
                        self.match(SparksqlParser.COMMA)
                        self.state = 203
                        self.table_source() 
                    self.state = 208
                    self._errHandler.sync(self)
                    _alt = self._interp.adaptivePredict(self._input,18,self._ctx)



            self.state = 213
            la_ = self._interp.adaptivePredict(self._input,20,self._ctx)
            if la_ == 1:
                self.state = 211
                self.match(SparksqlParser.WHERE)
                self.state = 212
                localctx.where = self.search_condition()


            self.state = 225
            la_ = self._interp.adaptivePredict(self._input,22,self._ctx)
            if la_ == 1:
                self.state = 215
                self.match(SparksqlParser.GROUP)
                self.state = 216
                self.match(SparksqlParser.BY)
                self.state = 217
                self.group_by_item()
                self.state = 222
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,21,self._ctx)
                while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                    if _alt==1:
                        self.state = 218
                        self.match(SparksqlParser.COMMA)
                        self.state = 219
                        self.group_by_item() 
                    self.state = 224
                    self._errHandler.sync(self)
                    _alt = self._interp.adaptivePredict(self._input,21,self._ctx)



            self.state = 229
            la_ = self._interp.adaptivePredict(self._input,23,self._ctx)
            if la_ == 1:
                self.state = 227
                self.match(SparksqlParser.HAVING)
                self.state = 228
                localctx.having = self.search_condition()


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class UnionContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def UNION(self):
            return self.getToken(SparksqlParser.UNION, 0)

        def EXCEPT(self):
            return self.getToken(SparksqlParser.EXCEPT, 0)

        def INTERSECT(self):
            return self.getToken(SparksqlParser.INTERSECT, 0)

        def query_specification(self):
            return self.getTypedRuleContext(SparksqlParser.Query_specificationContext,0)


        def ALL(self):
            return self.getToken(SparksqlParser.ALL, 0)

        def query_expression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.Query_expressionContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.Query_expressionContext,i)


        def getRuleIndex(self):
            return SparksqlParser.RULE_union

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterUnion" ):
                listener.enterUnion(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitUnion" ):
                listener.exitUnion(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitUnion" ):
                return visitor.visitUnion(self)
            else:
                return visitor.visitChildren(self)




    def union(self):

        localctx = SparksqlParser.UnionContext(self, self._ctx, self.state)
        self.enterRule(localctx, 20, self.RULE_union)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 237
            token = self._input.LA(1)
            if token in [SparksqlParser.UNION]:
                self.state = 231
                self.match(SparksqlParser.UNION)
                self.state = 233
                _la = self._input.LA(1)
                if _la==SparksqlParser.ALL:
                    self.state = 232
                    self.match(SparksqlParser.ALL)



            elif token in [SparksqlParser.EXCEPT]:
                self.state = 235
                self.match(SparksqlParser.EXCEPT)

            elif token in [SparksqlParser.INTERSECT]:
                self.state = 236
                self.match(SparksqlParser.INTERSECT)

            else:
                raise NoViableAltException(self)

            self.state = 248
            token = self._input.LA(1)
            if token in [SparksqlParser.SELECT]:
                self.state = 239
                self.query_specification()

            elif token in [SparksqlParser.LR_BRACKET]:
                self.state = 244 
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while True:
                    self.state = 240
                    self.match(SparksqlParser.LR_BRACKET)
                    self.state = 241
                    self.query_expression()
                    self.state = 242
                    self.match(SparksqlParser.RR_BRACKET)
                    self.state = 246 
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    if not (_la==SparksqlParser.LR_BRACKET):
                        break


            else:
                raise NoViableAltException(self)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Table_sourceContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def table_source_item_joined(self):
            return self.getTypedRuleContext(SparksqlParser.Table_source_item_joinedContext,0)


        def getRuleIndex(self):
            return SparksqlParser.RULE_table_source

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTable_source" ):
                listener.enterTable_source(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTable_source" ):
                listener.exitTable_source(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTable_source" ):
                return visitor.visitTable_source(self)
            else:
                return visitor.visitChildren(self)




    def table_source(self):

        localctx = SparksqlParser.Table_sourceContext(self, self._ctx, self.state)
        self.enterRule(localctx, 22, self.RULE_table_source)
        try:
            self.state = 255
            la_ = self._interp.adaptivePredict(self._input,28,self._ctx)
            if la_ == 1:
                self.enterOuterAlt(localctx, 1)
                self.state = 250
                self.table_source_item_joined()
                pass

            elif la_ == 2:
                self.enterOuterAlt(localctx, 2)
                self.state = 251
                self.match(SparksqlParser.LR_BRACKET)
                self.state = 252
                self.table_source_item_joined()
                self.state = 253
                self.match(SparksqlParser.RR_BRACKET)
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Table_source_item_joinedContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def table_source_item(self):
            return self.getTypedRuleContext(SparksqlParser.Table_source_itemContext,0)


        def join_part(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.Join_partContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.Join_partContext,i)


        def getRuleIndex(self):
            return SparksqlParser.RULE_table_source_item_joined

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTable_source_item_joined" ):
                listener.enterTable_source_item_joined(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTable_source_item_joined" ):
                listener.exitTable_source_item_joined(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTable_source_item_joined" ):
                return visitor.visitTable_source_item_joined(self)
            else:
                return visitor.visitChildren(self)




    def table_source_item_joined(self):

        localctx = SparksqlParser.Table_source_item_joinedContext(self, self._ctx, self.state)
        self.enterRule(localctx, 24, self.RULE_table_source_item_joined)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 257
            self.table_source_item()
            self.state = 261
            self._errHandler.sync(self)
            _alt = self._interp.adaptivePredict(self._input,29,self._ctx)
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt==1:
                    self.state = 258
                    self.join_part() 
                self.state = 263
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,29,self._ctx)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Table_source_itemContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def table_name_with_hint(self):
            return self.getTypedRuleContext(SparksqlParser.Table_name_with_hintContext,0)


        def as_table_alias(self):
            return self.getTypedRuleContext(SparksqlParser.As_table_aliasContext,0)


        def derived_table(self):
            return self.getTypedRuleContext(SparksqlParser.Derived_tableContext,0)


        def column_alias_list(self):
            return self.getTypedRuleContext(SparksqlParser.Column_alias_listContext,0)


        def LOCAL_ID(self):
            return self.getToken(SparksqlParser.LOCAL_ID, 0)

        def getRuleIndex(self):
            return SparksqlParser.RULE_table_source_item

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTable_source_item" ):
                listener.enterTable_source_item(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTable_source_item" ):
                listener.exitTable_source_item(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTable_source_item" ):
                return visitor.visitTable_source_item(self)
            else:
                return visitor.visitChildren(self)




    def table_source_item(self):

        localctx = SparksqlParser.Table_source_itemContext(self, self._ctx, self.state)
        self.enterRule(localctx, 26, self.RULE_table_source_item)
        self._la = 0 # Token type
        try:
            self.state = 279
            token = self._input.LA(1)
            if token in [SparksqlParser.ABSOLUTE, SparksqlParser.APPLY, SparksqlParser.AUTO, SparksqlParser.AVG, SparksqlParser.BASE64, SparksqlParser.CALLER, SparksqlParser.CAST, SparksqlParser.CATCH, SparksqlParser.CHECKSUM_AGG, SparksqlParser.COMMITTED, SparksqlParser.CONCAT, SparksqlParser.COOKIE, SparksqlParser.COUNT, SparksqlParser.COUNT_BIG, SparksqlParser.DELAY, SparksqlParser.DELETED, SparksqlParser.DENSE_RANK, SparksqlParser.DISABLE, SparksqlParser.DYNAMIC, SparksqlParser.ENCRYPTION, SparksqlParser.FAST, SparksqlParser.FAST_FORWARD, SparksqlParser.FIRST, SparksqlParser.FOLLOWING, SparksqlParser.FORWARD_ONLY, SparksqlParser.FULLSCAN, SparksqlParser.GLOBAL, SparksqlParser.GO, SparksqlParser.GROUPING, SparksqlParser.GROUPING_ID, SparksqlParser.HASH, SparksqlParser.INSENSITIVE, SparksqlParser.INSERTED, SparksqlParser.ISOLATION, SparksqlParser.KEEPFIXED, SparksqlParser.KEYSET, SparksqlParser.LAST, SparksqlParser.LEVEL, SparksqlParser.LOCAL, SparksqlParser.LOCK_ESCALATION, SparksqlParser.LOGIN, SparksqlParser.LOOP, SparksqlParser.MARK, SparksqlParser.MAX, SparksqlParser.MIN, SparksqlParser.MODIFY, SparksqlParser.NEXT, SparksqlParser.NAME, SparksqlParser.NOCOUNT, SparksqlParser.NOEXPAND, SparksqlParser.NORECOMPUTE, SparksqlParser.NTILE, SparksqlParser.NUMBER, SparksqlParser.OFFSET, SparksqlParser.ONLY, SparksqlParser.OPTIMISTIC, SparksqlParser.OPTIMIZE, SparksqlParser.OUT, SparksqlParser.OUTPUT, SparksqlParser.OWNER, SparksqlParser.PARTITION, SparksqlParser.PATH, SparksqlParser.PRECEDING, SparksqlParser.PRIOR, SparksqlParser.RANGE, SparksqlParser.RANK, SparksqlParser.READONLY, SparksqlParser.READ_ONLY, SparksqlParser.RECOMPILE, SparksqlParser.RELATIVE, SparksqlParser.REMOTE, SparksqlParser.REPEATABLE, SparksqlParser.ROOT, SparksqlParser.ROW, SparksqlParser.ROWGUID, SparksqlParser.ROWS, SparksqlParser.ROW_NUMBER, SparksqlParser.SAMPLE, SparksqlParser.SCHEMABINDING, SparksqlParser.SCROLL, SparksqlParser.SCROLL_LOCKS, SparksqlParser.SELF, SparksqlParser.SERIALIZABLE, SparksqlParser.SNAPSHOT, SparksqlParser.STATIC, SparksqlParser.STATS_STREAM, SparksqlParser.STDEV, SparksqlParser.STDEVP, SparksqlParser.SUM, SparksqlParser.THROW, SparksqlParser.TIES, SparksqlParser.TIME, SparksqlParser.TRY, SparksqlParser.TYPE, SparksqlParser.TYPE_WARNING, SparksqlParser.UNBOUNDED, SparksqlParser.UNCOMMITTED, SparksqlParser.UNKNOWN, SparksqlParser.USING, SparksqlParser.VAR, SparksqlParser.VARP, SparksqlParser.VIEW_METADATA, SparksqlParser.WORK, SparksqlParser.XML, SparksqlParser.XMLNAMESPACES, SparksqlParser.DOUBLE_QUOTE_ID, SparksqlParser.SQUARE_BRACKET_ID, SparksqlParser.ID, SparksqlParser.DOT]:
                self.enterOuterAlt(localctx, 1)
                self.state = 264
                self.table_name_with_hint()
                self.state = 266
                la_ = self._interp.adaptivePredict(self._input,30,self._ctx)
                if la_ == 1:
                    self.state = 265
                    self.as_table_alias()



            elif token in [SparksqlParser.SELECT, SparksqlParser.VALUES, SparksqlParser.WITH, SparksqlParser.LR_BRACKET]:
                self.enterOuterAlt(localctx, 2)
                self.state = 268
                self.derived_table()
                self.state = 273
                la_ = self._interp.adaptivePredict(self._input,32,self._ctx)
                if la_ == 1:
                    self.state = 269
                    self.as_table_alias()
                    self.state = 271
                    _la = self._input.LA(1)
                    if _la==SparksqlParser.LR_BRACKET:
                        self.state = 270
                        self.column_alias_list()





            elif token in [SparksqlParser.LOCAL_ID]:
                self.enterOuterAlt(localctx, 3)
                self.state = 275
                self.match(SparksqlParser.LOCAL_ID)
                self.state = 277
                la_ = self._interp.adaptivePredict(self._input,33,self._ctx)
                if la_ == 1:
                    self.state = 276
                    self.as_table_alias()



            else:
                raise NoViableAltException(self)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Group_by_itemContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def expression(self):
            return self.getTypedRuleContext(SparksqlParser.ExpressionContext,0)


        def getRuleIndex(self):
            return SparksqlParser.RULE_group_by_item

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterGroup_by_item" ):
                listener.enterGroup_by_item(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitGroup_by_item" ):
                listener.exitGroup_by_item(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitGroup_by_item" ):
                return visitor.visitGroup_by_item(self)
            else:
                return visitor.visitChildren(self)




    def group_by_item(self):

        localctx = SparksqlParser.Group_by_itemContext(self, self._ctx, self.state)
        self.enterRule(localctx, 28, self.RULE_group_by_item)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 281
            self.expression(0)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Derived_tableContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def subquery(self):
            return self.getTypedRuleContext(SparksqlParser.SubqueryContext,0)


        def table_value_constructor(self):
            return self.getTypedRuleContext(SparksqlParser.Table_value_constructorContext,0)


        def getRuleIndex(self):
            return SparksqlParser.RULE_derived_table

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterDerived_table" ):
                listener.enterDerived_table(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitDerived_table" ):
                listener.exitDerived_table(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitDerived_table" ):
                return visitor.visitDerived_table(self)
            else:
                return visitor.visitChildren(self)




    def derived_table(self):

        localctx = SparksqlParser.Derived_tableContext(self, self._ctx, self.state)
        self.enterRule(localctx, 30, self.RULE_derived_table)
        try:
            self.state = 289
            la_ = self._interp.adaptivePredict(self._input,35,self._ctx)
            if la_ == 1:
                self.enterOuterAlt(localctx, 1)
                self.state = 283
                self.subquery()
                pass

            elif la_ == 2:
                self.enterOuterAlt(localctx, 2)
                self.state = 284
                self.match(SparksqlParser.LR_BRACKET)
                self.state = 285
                self.subquery()
                self.state = 286
                self.match(SparksqlParser.RR_BRACKET)
                pass

            elif la_ == 3:
                self.enterOuterAlt(localctx, 3)
                self.state = 288
                self.table_value_constructor()
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Table_value_constructorContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def VALUES(self):
            return self.getToken(SparksqlParser.VALUES, 0)

        def expression_list(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.Expression_listContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.Expression_listContext,i)


        def getRuleIndex(self):
            return SparksqlParser.RULE_table_value_constructor

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTable_value_constructor" ):
                listener.enterTable_value_constructor(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTable_value_constructor" ):
                listener.exitTable_value_constructor(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTable_value_constructor" ):
                return visitor.visitTable_value_constructor(self)
            else:
                return visitor.visitChildren(self)




    def table_value_constructor(self):

        localctx = SparksqlParser.Table_value_constructorContext(self, self._ctx, self.state)
        self.enterRule(localctx, 32, self.RULE_table_value_constructor)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 291
            self.match(SparksqlParser.VALUES)
            self.state = 292
            self.match(SparksqlParser.LR_BRACKET)
            self.state = 293
            self.expression_list()
            self.state = 294
            self.match(SparksqlParser.RR_BRACKET)
            self.state = 302
            self._errHandler.sync(self)
            _alt = self._interp.adaptivePredict(self._input,36,self._ctx)
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt==1:
                    self.state = 295
                    self.match(SparksqlParser.COMMA)
                    self.state = 296
                    self.match(SparksqlParser.LR_BRACKET)
                    self.state = 297
                    self.expression_list()
                    self.state = 298
                    self.match(SparksqlParser.RR_BRACKET) 
                self.state = 304
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,36,self._ctx)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Expression_listContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def expression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.ExpressionContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.ExpressionContext,i)


        def getRuleIndex(self):
            return SparksqlParser.RULE_expression_list

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterExpression_list" ):
                listener.enterExpression_list(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitExpression_list" ):
                listener.exitExpression_list(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitExpression_list" ):
                return visitor.visitExpression_list(self)
            else:
                return visitor.visitChildren(self)




    def expression_list(self):

        localctx = SparksqlParser.Expression_listContext(self, self._ctx, self.state)
        self.enterRule(localctx, 34, self.RULE_expression_list)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 305
            self.expression(0)
            self.state = 310
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparksqlParser.COMMA:
                self.state = 306
                self.match(SparksqlParser.COMMA)
                self.state = 307
                self.expression(0)
                self.state = 312
                self._errHandler.sync(self)
                _la = self._input.LA(1)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class SubqueryContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def select_statement(self):
            return self.getTypedRuleContext(SparksqlParser.Select_statementContext,0)


        def getRuleIndex(self):
            return SparksqlParser.RULE_subquery

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSubquery" ):
                listener.enterSubquery(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSubquery" ):
                listener.exitSubquery(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSubquery" ):
                return visitor.visitSubquery(self)
            else:
                return visitor.visitChildren(self)




    def subquery(self):

        localctx = SparksqlParser.SubqueryContext(self, self._ctx, self.state)
        self.enterRule(localctx, 36, self.RULE_subquery)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 313
            self.select_statement()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Join_partContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.join_type = None # Token
            self.join_hint = None # Token

        def JOIN(self):
            return self.getToken(SparksqlParser.JOIN, 0)

        def table_source(self):
            return self.getTypedRuleContext(SparksqlParser.Table_sourceContext,0)


        def ON(self):
            return self.getToken(SparksqlParser.ON, 0)

        def search_condition(self):
            return self.getTypedRuleContext(SparksqlParser.Search_conditionContext,0)


        def LEFT(self):
            return self.getToken(SparksqlParser.LEFT, 0)

        def RIGHT(self):
            return self.getToken(SparksqlParser.RIGHT, 0)

        def FULL(self):
            return self.getToken(SparksqlParser.FULL, 0)

        def INNER(self):
            return self.getToken(SparksqlParser.INNER, 0)

        def OUTER(self):
            return self.getToken(SparksqlParser.OUTER, 0)

        def LOOP(self):
            return self.getToken(SparksqlParser.LOOP, 0)

        def HASH(self):
            return self.getToken(SparksqlParser.HASH, 0)

        def MERGE(self):
            return self.getToken(SparksqlParser.MERGE, 0)

        def REMOTE(self):
            return self.getToken(SparksqlParser.REMOTE, 0)

        def CROSS(self):
            return self.getToken(SparksqlParser.CROSS, 0)

        def APPLY(self):
            return self.getToken(SparksqlParser.APPLY, 0)

        def getRuleIndex(self):
            return SparksqlParser.RULE_join_part

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterJoin_part" ):
                listener.enterJoin_part(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitJoin_part" ):
                listener.exitJoin_part(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitJoin_part" ):
                return visitor.visitJoin_part(self)
            else:
                return visitor.visitChildren(self)




    def join_part(self):

        localctx = SparksqlParser.Join_partContext(self, self._ctx, self.state)
        self.enterRule(localctx, 38, self.RULE_join_part)
        self._la = 0 # Token type
        try:
            self.state = 341
            la_ = self._interp.adaptivePredict(self._input,42,self._ctx)
            if la_ == 1:
                self.enterOuterAlt(localctx, 1)
                self.state = 322
                token = self._input.LA(1)
                if token in [SparksqlParser.INNER, SparksqlParser.JOIN, SparksqlParser.MERGE, SparksqlParser.HASH, SparksqlParser.LOOP, SparksqlParser.REMOTE]:
                    self.state = 316
                    _la = self._input.LA(1)
                    if _la==SparksqlParser.INNER:
                        self.state = 315
                        self.match(SparksqlParser.INNER)



                elif token in [SparksqlParser.FULL, SparksqlParser.LEFT, SparksqlParser.RIGHT]:
                    self.state = 318
                    localctx.join_type = self._input.LT(1)
                    _la = self._input.LA(1)
                    if not(_la==SparksqlParser.FULL or _la==SparksqlParser.LEFT or _la==SparksqlParser.RIGHT):
                        localctx.join_type = self._errHandler.recoverInline(self)
                    else:
                        self.consume()
                    self.state = 320
                    _la = self._input.LA(1)
                    if _la==SparksqlParser.OUTER:
                        self.state = 319
                        self.match(SparksqlParser.OUTER)



                else:
                    raise NoViableAltException(self)

                self.state = 325
                _la = self._input.LA(1)
                if _la==SparksqlParser.MERGE or ((((_la - 217)) & ~0x3f) == 0 and ((1 << (_la - 217)) & ((1 << (SparksqlParser.HASH - 217)) | (1 << (SparksqlParser.LOOP - 217)) | (1 << (SparksqlParser.REMOTE - 217)))) != 0):
                    self.state = 324
                    localctx.join_hint = self._input.LT(1)
                    _la = self._input.LA(1)
                    if not(_la==SparksqlParser.MERGE or ((((_la - 217)) & ~0x3f) == 0 and ((1 << (_la - 217)) & ((1 << (SparksqlParser.HASH - 217)) | (1 << (SparksqlParser.LOOP - 217)) | (1 << (SparksqlParser.REMOTE - 217)))) != 0)):
                        localctx.join_hint = self._errHandler.recoverInline(self)
                    else:
                        self.consume()


                self.state = 327
                self.match(SparksqlParser.JOIN)
                self.state = 328
                self.table_source()
                self.state = 329
                self.match(SparksqlParser.ON)
                self.state = 330
                self.search_condition()
                pass

            elif la_ == 2:
                self.enterOuterAlt(localctx, 2)
                self.state = 332
                self.match(SparksqlParser.CROSS)
                self.state = 333
                self.match(SparksqlParser.JOIN)
                self.state = 334
                self.table_source()
                pass

            elif la_ == 3:
                self.enterOuterAlt(localctx, 3)
                self.state = 335
                self.match(SparksqlParser.CROSS)
                self.state = 336
                self.match(SparksqlParser.APPLY)
                self.state = 337
                self.table_source()
                pass

            elif la_ == 4:
                self.enterOuterAlt(localctx, 4)
                self.state = 338
                self.match(SparksqlParser.OUTER)
                self.state = 339
                self.match(SparksqlParser.APPLY)
                self.state = 340
                self.table_source()
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Table_name_with_hintContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def table_name(self):
            return self.getTypedRuleContext(SparksqlParser.Table_nameContext,0)


        def with_table_hints(self):
            return self.getTypedRuleContext(SparksqlParser.With_table_hintsContext,0)


        def getRuleIndex(self):
            return SparksqlParser.RULE_table_name_with_hint

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTable_name_with_hint" ):
                listener.enterTable_name_with_hint(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTable_name_with_hint" ):
                listener.exitTable_name_with_hint(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTable_name_with_hint" ):
                return visitor.visitTable_name_with_hint(self)
            else:
                return visitor.visitChildren(self)




    def table_name_with_hint(self):

        localctx = SparksqlParser.Table_name_with_hintContext(self, self._ctx, self.state)
        self.enterRule(localctx, 40, self.RULE_table_name_with_hint)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 343
            self.table_name()
            self.state = 345
            _la = self._input.LA(1)
            if _la==SparksqlParser.WITH or _la==SparksqlParser.LR_BRACKET:
                self.state = 344
                self.with_table_hints()


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class As_table_aliasContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def table_alias(self):
            return self.getTypedRuleContext(SparksqlParser.Table_aliasContext,0)


        def AS(self):
            return self.getToken(SparksqlParser.AS, 0)

        def getRuleIndex(self):
            return SparksqlParser.RULE_as_table_alias

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterAs_table_alias" ):
                listener.enterAs_table_alias(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitAs_table_alias" ):
                listener.exitAs_table_alias(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitAs_table_alias" ):
                return visitor.visitAs_table_alias(self)
            else:
                return visitor.visitChildren(self)




    def as_table_alias(self):

        localctx = SparksqlParser.As_table_aliasContext(self, self._ctx, self.state)
        self.enterRule(localctx, 42, self.RULE_as_table_alias)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 348
            _la = self._input.LA(1)
            if _la==SparksqlParser.AS:
                self.state = 347
                self.match(SparksqlParser.AS)


            self.state = 350
            self.table_alias()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Table_aliasContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def id_1(self):
            return self.getTypedRuleContext(SparksqlParser.Id_1Context,0)


        def with_table_hints(self):
            return self.getTypedRuleContext(SparksqlParser.With_table_hintsContext,0)


        def getRuleIndex(self):
            return SparksqlParser.RULE_table_alias

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTable_alias" ):
                listener.enterTable_alias(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTable_alias" ):
                listener.exitTable_alias(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTable_alias" ):
                return visitor.visitTable_alias(self)
            else:
                return visitor.visitChildren(self)




    def table_alias(self):

        localctx = SparksqlParser.Table_aliasContext(self, self._ctx, self.state)
        self.enterRule(localctx, 44, self.RULE_table_alias)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 352
            self.id_1()
            self.state = 354
            la_ = self._interp.adaptivePredict(self._input,45,self._ctx)
            if la_ == 1:
                self.state = 353
                self.with_table_hints()


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class With_table_hintsContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def table_hint(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.Table_hintContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.Table_hintContext,i)


        def WITH(self):
            return self.getToken(SparksqlParser.WITH, 0)

        def getRuleIndex(self):
            return SparksqlParser.RULE_with_table_hints

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterWith_table_hints" ):
                listener.enterWith_table_hints(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitWith_table_hints" ):
                listener.exitWith_table_hints(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitWith_table_hints" ):
                return visitor.visitWith_table_hints(self)
            else:
                return visitor.visitChildren(self)




    def with_table_hints(self):

        localctx = SparksqlParser.With_table_hintsContext(self, self._ctx, self.state)
        self.enterRule(localctx, 46, self.RULE_with_table_hints)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 357
            _la = self._input.LA(1)
            if _la==SparksqlParser.WITH:
                self.state = 356
                self.match(SparksqlParser.WITH)


            self.state = 359
            self.match(SparksqlParser.LR_BRACKET)
            self.state = 360
            self.table_hint()
            self.state = 365
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparksqlParser.COMMA:
                self.state = 361
                self.match(SparksqlParser.COMMA)
                self.state = 362
                self.table_hint()
                self.state = 367
                self._errHandler.sync(self)
                _la = self._input.LA(1)

            self.state = 368
            self.match(SparksqlParser.RR_BRACKET)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Table_hintContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def INDEX(self):
            return self.getToken(SparksqlParser.INDEX, 0)

        def index_value(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.Index_valueContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.Index_valueContext,i)


        def ID(self):
            return self.getToken(SparksqlParser.ID, 0)

        def NOEXPAND(self):
            return self.getToken(SparksqlParser.NOEXPAND, 0)

        def getRuleIndex(self):
            return SparksqlParser.RULE_table_hint

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTable_hint" ):
                listener.enterTable_hint(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTable_hint" ):
                listener.exitTable_hint(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTable_hint" ):
                return visitor.visitTable_hint(self)
            else:
                return visitor.visitChildren(self)




    def table_hint(self):

        localctx = SparksqlParser.Table_hintContext(self, self._ctx, self.state)
        self.enterRule(localctx, 48, self.RULE_table_hint)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 371
            _la = self._input.LA(1)
            if _la==SparksqlParser.NOEXPAND:
                self.state = 370
                self.match(SparksqlParser.NOEXPAND)


            self.state = 389
            la_ = self._interp.adaptivePredict(self._input,50,self._ctx)
            if la_ == 1:
                self.state = 373
                self.match(SparksqlParser.INDEX)
                self.state = 374
                self.match(SparksqlParser.LR_BRACKET)
                self.state = 375
                self.index_value()
                self.state = 380
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while _la==SparksqlParser.COMMA:
                    self.state = 376
                    self.match(SparksqlParser.COMMA)
                    self.state = 377
                    self.index_value()
                    self.state = 382
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)

                self.state = 383
                self.match(SparksqlParser.RR_BRACKET)
                pass

            elif la_ == 2:
                self.state = 385
                self.match(SparksqlParser.INDEX)
                self.state = 386
                self.match(SparksqlParser.EQUAL)
                self.state = 387
                self.index_value()
                pass

            elif la_ == 3:
                self.state = 388
                self.match(SparksqlParser.ID)
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Index_valueContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def ID(self):
            return self.getToken(SparksqlParser.ID, 0)

        def DECIMAL(self):
            return self.getToken(SparksqlParser.DECIMAL, 0)

        def getRuleIndex(self):
            return SparksqlParser.RULE_index_value

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterIndex_value" ):
                listener.enterIndex_value(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitIndex_value" ):
                listener.exitIndex_value(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitIndex_value" ):
                return visitor.visitIndex_value(self)
            else:
                return visitor.visitChildren(self)




    def index_value(self):

        localctx = SparksqlParser.Index_valueContext(self, self._ctx, self.state)
        self.enterRule(localctx, 50, self.RULE_index_value)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 391
            _la = self._input.LA(1)
            if not(_la==SparksqlParser.DECIMAL or _la==SparksqlParser.ID):
                self._errHandler.recoverInline(self)
            else:
                self.consume()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Column_alias_listContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def column_alias(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.Column_aliasContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.Column_aliasContext,i)


        def getRuleIndex(self):
            return SparksqlParser.RULE_column_alias_list

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterColumn_alias_list" ):
                listener.enterColumn_alias_list(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitColumn_alias_list" ):
                listener.exitColumn_alias_list(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitColumn_alias_list" ):
                return visitor.visitColumn_alias_list(self)
            else:
                return visitor.visitChildren(self)




    def column_alias_list(self):

        localctx = SparksqlParser.Column_alias_listContext(self, self._ctx, self.state)
        self.enterRule(localctx, 52, self.RULE_column_alias_list)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 393
            self.match(SparksqlParser.LR_BRACKET)
            self.state = 394
            self.column_alias()
            self.state = 399
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparksqlParser.COMMA:
                self.state = 395
                self.match(SparksqlParser.COMMA)
                self.state = 396
                self.column_alias()
                self.state = 401
                self._errHandler.sync(self)
                _la = self._input.LA(1)

            self.state = 402
            self.match(SparksqlParser.RR_BRACKET)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Column_aliasContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def id_1(self):
            return self.getTypedRuleContext(SparksqlParser.Id_1Context,0)


        def STRING(self):
            return self.getToken(SparksqlParser.STRING, 0)

        def getRuleIndex(self):
            return SparksqlParser.RULE_column_alias

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterColumn_alias" ):
                listener.enterColumn_alias(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitColumn_alias" ):
                listener.exitColumn_alias(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitColumn_alias" ):
                return visitor.visitColumn_alias(self)
            else:
                return visitor.visitChildren(self)




    def column_alias(self):

        localctx = SparksqlParser.Column_aliasContext(self, self._ctx, self.state)
        self.enterRule(localctx, 54, self.RULE_column_alias)
        try:
            self.state = 406
            token = self._input.LA(1)
            if token in [SparksqlParser.ABSOLUTE, SparksqlParser.APPLY, SparksqlParser.AUTO, SparksqlParser.AVG, SparksqlParser.BASE64, SparksqlParser.CALLER, SparksqlParser.CAST, SparksqlParser.CATCH, SparksqlParser.CHECKSUM_AGG, SparksqlParser.COMMITTED, SparksqlParser.CONCAT, SparksqlParser.COOKIE, SparksqlParser.COUNT, SparksqlParser.COUNT_BIG, SparksqlParser.DELAY, SparksqlParser.DELETED, SparksqlParser.DENSE_RANK, SparksqlParser.DISABLE, SparksqlParser.DYNAMIC, SparksqlParser.ENCRYPTION, SparksqlParser.FAST, SparksqlParser.FAST_FORWARD, SparksqlParser.FIRST, SparksqlParser.FOLLOWING, SparksqlParser.FORWARD_ONLY, SparksqlParser.FULLSCAN, SparksqlParser.GLOBAL, SparksqlParser.GO, SparksqlParser.GROUPING, SparksqlParser.GROUPING_ID, SparksqlParser.HASH, SparksqlParser.INSENSITIVE, SparksqlParser.INSERTED, SparksqlParser.ISOLATION, SparksqlParser.KEEPFIXED, SparksqlParser.KEYSET, SparksqlParser.LAST, SparksqlParser.LEVEL, SparksqlParser.LOCAL, SparksqlParser.LOCK_ESCALATION, SparksqlParser.LOGIN, SparksqlParser.LOOP, SparksqlParser.MARK, SparksqlParser.MAX, SparksqlParser.MIN, SparksqlParser.MODIFY, SparksqlParser.NEXT, SparksqlParser.NAME, SparksqlParser.NOCOUNT, SparksqlParser.NOEXPAND, SparksqlParser.NORECOMPUTE, SparksqlParser.NTILE, SparksqlParser.NUMBER, SparksqlParser.OFFSET, SparksqlParser.ONLY, SparksqlParser.OPTIMISTIC, SparksqlParser.OPTIMIZE, SparksqlParser.OUT, SparksqlParser.OUTPUT, SparksqlParser.OWNER, SparksqlParser.PARTITION, SparksqlParser.PATH, SparksqlParser.PRECEDING, SparksqlParser.PRIOR, SparksqlParser.RANGE, SparksqlParser.RANK, SparksqlParser.READONLY, SparksqlParser.READ_ONLY, SparksqlParser.RECOMPILE, SparksqlParser.RELATIVE, SparksqlParser.REMOTE, SparksqlParser.REPEATABLE, SparksqlParser.ROOT, SparksqlParser.ROW, SparksqlParser.ROWGUID, SparksqlParser.ROWS, SparksqlParser.ROW_NUMBER, SparksqlParser.SAMPLE, SparksqlParser.SCHEMABINDING, SparksqlParser.SCROLL, SparksqlParser.SCROLL_LOCKS, SparksqlParser.SELF, SparksqlParser.SERIALIZABLE, SparksqlParser.SNAPSHOT, SparksqlParser.STATIC, SparksqlParser.STATS_STREAM, SparksqlParser.STDEV, SparksqlParser.STDEVP, SparksqlParser.SUM, SparksqlParser.THROW, SparksqlParser.TIES, SparksqlParser.TIME, SparksqlParser.TRY, SparksqlParser.TYPE, SparksqlParser.TYPE_WARNING, SparksqlParser.UNBOUNDED, SparksqlParser.UNCOMMITTED, SparksqlParser.UNKNOWN, SparksqlParser.USING, SparksqlParser.VAR, SparksqlParser.VARP, SparksqlParser.VIEW_METADATA, SparksqlParser.WORK, SparksqlParser.XML, SparksqlParser.XMLNAMESPACES, SparksqlParser.DOUBLE_QUOTE_ID, SparksqlParser.SQUARE_BRACKET_ID, SparksqlParser.ID]:
                self.enterOuterAlt(localctx, 1)
                self.state = 404
                self.id_1()

            elif token in [SparksqlParser.STRING]:
                self.enterOuterAlt(localctx, 2)
                self.state = 405
                self.match(SparksqlParser.STRING)

            else:
                raise NoViableAltException(self)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class ExpressionContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.op = None # Token

        def expression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.ExpressionContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.ExpressionContext,i)


        def DEFAULT(self):
            return self.getToken(SparksqlParser.DEFAULT, 0)

        def NULL(self):
            return self.getToken(SparksqlParser.NULL, 0)

        def LOCAL_ID(self):
            return self.getToken(SparksqlParser.LOCAL_ID, 0)

        def constant(self):
            return self.getTypedRuleContext(SparksqlParser.ConstantContext,0)


        def case_expr(self):
            return self.getTypedRuleContext(SparksqlParser.Case_exprContext,0)


        def full_column_name(self):
            return self.getTypedRuleContext(SparksqlParser.Full_column_nameContext,0)


        def subquery(self):
            return self.getTypedRuleContext(SparksqlParser.SubqueryContext,0)


        def comparison_operator(self):
            return self.getTypedRuleContext(SparksqlParser.Comparison_operatorContext,0)


        def COLLATE(self):
            return self.getToken(SparksqlParser.COLLATE, 0)

        def id_1(self):
            return self.getTypedRuleContext(SparksqlParser.Id_1Context,0)


        def getRuleIndex(self):
            return SparksqlParser.RULE_expression

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterExpression" ):
                listener.enterExpression(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitExpression" ):
                listener.exitExpression(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitExpression" ):
                return visitor.visitExpression(self)
            else:
                return visitor.visitChildren(self)



    def expression(self, _p:int=0):
        _parentctx = self._ctx
        _parentState = self.state
        localctx = SparksqlParser.ExpressionContext(self, self._ctx, _parentState)
        _prevctx = localctx
        _startState = 56
        self.enterRecursionRule(localctx, 56, self.RULE_expression, _p)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 427
            la_ = self._interp.adaptivePredict(self._input,53,self._ctx)
            if la_ == 1:
                self.state = 409
                self.match(SparksqlParser.BIT_NOT)
                self.state = 410
                self.expression(5)
                pass

            elif la_ == 2:
                self.state = 411
                localctx.op = self._input.LT(1)
                _la = self._input.LA(1)
                if not(_la==SparksqlParser.PLUS or _la==SparksqlParser.MINUS):
                    localctx.op = self._errHandler.recoverInline(self)
                else:
                    self.consume()
                self.state = 412
                self.expression(3)
                pass

            elif la_ == 3:
                self.state = 413
                self.match(SparksqlParser.DEFAULT)
                pass

            elif la_ == 4:
                self.state = 414
                self.match(SparksqlParser.NULL)
                pass

            elif la_ == 5:
                self.state = 415
                self.match(SparksqlParser.LOCAL_ID)
                pass

            elif la_ == 6:
                self.state = 416
                self.constant()
                pass

            elif la_ == 7:
                self.state = 417
                self.case_expr()
                pass

            elif la_ == 8:
                self.state = 418
                self.full_column_name()
                pass

            elif la_ == 9:
                self.state = 419
                self.match(SparksqlParser.LR_BRACKET)
                self.state = 420
                self.expression(0)
                self.state = 421
                self.match(SparksqlParser.RR_BRACKET)
                pass

            elif la_ == 10:
                self.state = 423
                self.match(SparksqlParser.LR_BRACKET)
                self.state = 424
                self.subquery()
                self.state = 425
                self.match(SparksqlParser.RR_BRACKET)
                pass


            self._ctx.stop = self._input.LT(-1)
            self.state = 444
            self._errHandler.sync(self)
            _alt = self._interp.adaptivePredict(self._input,55,self._ctx)
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt==1:
                    if self._parseListeners is not None:
                        self.triggerExitRuleEvent()
                    _prevctx = localctx
                    self.state = 442
                    la_ = self._interp.adaptivePredict(self._input,54,self._ctx)
                    if la_ == 1:
                        localctx = SparksqlParser.ExpressionContext(self, _parentctx, _parentState)
                        self.pushNewRecursionContext(localctx, _startState, self.RULE_expression)
                        self.state = 429
                        if not self.precpred(self._ctx, 4):
                            from antlr4.error.Errors import FailedPredicateException
                            raise FailedPredicateException(self, "self.precpred(self._ctx, 4)")
                        self.state = 430
                        localctx.op = self._input.LT(1)
                        _la = self._input.LA(1)
                        if not(((((_la - 327)) & ~0x3f) == 0 and ((1 << (_la - 327)) & ((1 << (SparksqlParser.STAR - 327)) | (1 << (SparksqlParser.DIVIDE - 327)) | (1 << (SparksqlParser.MODULE - 327)))) != 0)):
                            localctx.op = self._errHandler.recoverInline(self)
                        else:
                            self.consume()
                        self.state = 431
                        self.expression(5)
                        pass

                    elif la_ == 2:
                        localctx = SparksqlParser.ExpressionContext(self, _parentctx, _parentState)
                        self.pushNewRecursionContext(localctx, _startState, self.RULE_expression)
                        self.state = 432
                        if not self.precpred(self._ctx, 2):
                            from antlr4.error.Errors import FailedPredicateException
                            raise FailedPredicateException(self, "self.precpred(self._ctx, 2)")
                        self.state = 433
                        localctx.op = self._input.LT(1)
                        _la = self._input.LA(1)
                        if not(((((_la - 330)) & ~0x3f) == 0 and ((1 << (_la - 330)) & ((1 << (SparksqlParser.PLUS - 330)) | (1 << (SparksqlParser.MINUS - 330)) | (1 << (SparksqlParser.BIT_OR - 330)) | (1 << (SparksqlParser.BIT_AND - 330)) | (1 << (SparksqlParser.BIT_XOR - 330)))) != 0)):
                            localctx.op = self._errHandler.recoverInline(self)
                        else:
                            self.consume()
                        self.state = 434
                        self.expression(3)
                        pass

                    elif la_ == 3:
                        localctx = SparksqlParser.ExpressionContext(self, _parentctx, _parentState)
                        self.pushNewRecursionContext(localctx, _startState, self.RULE_expression)
                        self.state = 435
                        if not self.precpred(self._ctx, 1):
                            from antlr4.error.Errors import FailedPredicateException
                            raise FailedPredicateException(self, "self.precpred(self._ctx, 1)")
                        self.state = 436
                        self.comparison_operator()
                        self.state = 437
                        self.expression(2)
                        pass

                    elif la_ == 4:
                        localctx = SparksqlParser.ExpressionContext(self, _parentctx, _parentState)
                        self.pushNewRecursionContext(localctx, _startState, self.RULE_expression)
                        self.state = 439
                        if not self.precpred(self._ctx, 10):
                            from antlr4.error.Errors import FailedPredicateException
                            raise FailedPredicateException(self, "self.precpred(self._ctx, 10)")
                        self.state = 440
                        self.match(SparksqlParser.COLLATE)
                        self.state = 441
                        self.id_1()
                        pass

             
                self.state = 446
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,55,self._ctx)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.unrollRecursionContexts(_parentctx)
        return localctx

    class ConstantContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def STRING(self):
            return self.getToken(SparksqlParser.STRING, 0)

        def BINARY(self):
            return self.getToken(SparksqlParser.BINARY, 0)

        def number(self):
            return self.getTypedRuleContext(SparksqlParser.NumberContext,0)


        def REAL(self):
            return self.getToken(SparksqlParser.REAL, 0)

        def FLOAT(self):
            return self.getToken(SparksqlParser.FLOAT, 0)

        def sign(self):
            return self.getTypedRuleContext(SparksqlParser.SignContext,0)


        def DECIMAL(self):
            return self.getToken(SparksqlParser.DECIMAL, 0)

        def getRuleIndex(self):
            return SparksqlParser.RULE_constant

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterConstant" ):
                listener.enterConstant(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitConstant" ):
                listener.exitConstant(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitConstant" ):
                return visitor.visitConstant(self)
            else:
                return visitor.visitChildren(self)




    def constant(self):

        localctx = SparksqlParser.ConstantContext(self, self._ctx, self.state)
        self.enterRule(localctx, 58, self.RULE_constant)
        self._la = 0 # Token type
        try:
            self.state = 459
            la_ = self._interp.adaptivePredict(self._input,58,self._ctx)
            if la_ == 1:
                self.enterOuterAlt(localctx, 1)
                self.state = 447
                self.match(SparksqlParser.STRING)
                pass

            elif la_ == 2:
                self.enterOuterAlt(localctx, 2)
                self.state = 448
                self.match(SparksqlParser.BINARY)
                pass

            elif la_ == 3:
                self.enterOuterAlt(localctx, 3)
                self.state = 449
                self.number()
                pass

            elif la_ == 4:
                self.enterOuterAlt(localctx, 4)
                self.state = 451
                _la = self._input.LA(1)
                if _la==SparksqlParser.PLUS or _la==SparksqlParser.MINUS:
                    self.state = 450
                    self.sign()


                self.state = 453
                _la = self._input.LA(1)
                if not(_la==SparksqlParser.FLOAT or _la==SparksqlParser.REAL):
                    self._errHandler.recoverInline(self)
                else:
                    self.consume()
                pass

            elif la_ == 5:
                self.enterOuterAlt(localctx, 5)
                self.state = 455
                _la = self._input.LA(1)
                if _la==SparksqlParser.PLUS or _la==SparksqlParser.MINUS:
                    self.state = 454
                    self.sign()


                self.state = 457
                self.match(SparksqlParser.DOLLAR)
                self.state = 458
                _la = self._input.LA(1)
                if not(_la==SparksqlParser.DECIMAL or _la==SparksqlParser.FLOAT):
                    self._errHandler.recoverInline(self)
                else:
                    self.consume()
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class SignContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def PLUS(self):
            return self.getToken(SparksqlParser.PLUS, 0)

        def MINUS(self):
            return self.getToken(SparksqlParser.MINUS, 0)

        def getRuleIndex(self):
            return SparksqlParser.RULE_sign

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSign" ):
                listener.enterSign(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSign" ):
                listener.exitSign(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSign" ):
                return visitor.visitSign(self)
            else:
                return visitor.visitChildren(self)




    def sign(self):

        localctx = SparksqlParser.SignContext(self, self._ctx, self.state)
        self.enterRule(localctx, 60, self.RULE_sign)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 461
            _la = self._input.LA(1)
            if not(_la==SparksqlParser.PLUS or _la==SparksqlParser.MINUS):
                self._errHandler.recoverInline(self)
            else:
                self.consume()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class NumberContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def DECIMAL(self):
            return self.getToken(SparksqlParser.DECIMAL, 0)

        def sign(self):
            return self.getTypedRuleContext(SparksqlParser.SignContext,0)


        def getRuleIndex(self):
            return SparksqlParser.RULE_number

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterNumber" ):
                listener.enterNumber(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitNumber" ):
                listener.exitNumber(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitNumber" ):
                return visitor.visitNumber(self)
            else:
                return visitor.visitChildren(self)




    def number(self):

        localctx = SparksqlParser.NumberContext(self, self._ctx, self.state)
        self.enterRule(localctx, 62, self.RULE_number)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 464
            _la = self._input.LA(1)
            if _la==SparksqlParser.PLUS or _la==SparksqlParser.MINUS:
                self.state = 463
                self.sign()


            self.state = 466
            self.match(SparksqlParser.DECIMAL)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Select_listContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def select_list_elem(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.Select_list_elemContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.Select_list_elemContext,i)


        def getRuleIndex(self):
            return SparksqlParser.RULE_select_list

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSelect_list" ):
                listener.enterSelect_list(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSelect_list" ):
                listener.exitSelect_list(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSelect_list" ):
                return visitor.visitSelect_list(self)
            else:
                return visitor.visitChildren(self)




    def select_list(self):

        localctx = SparksqlParser.Select_listContext(self, self._ctx, self.state)
        self.enterRule(localctx, 64, self.RULE_select_list)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 468
            self.select_list_elem()
            self.state = 473
            self._errHandler.sync(self)
            _alt = self._interp.adaptivePredict(self._input,60,self._ctx)
            while _alt!=2 and _alt!=ATN.INVALID_ALT_NUMBER:
                if _alt==1:
                    self.state = 469
                    self.match(SparksqlParser.COMMA)
                    self.state = 470
                    self.select_list_elem() 
                self.state = 475
                self._errHandler.sync(self)
                _alt = self._interp.adaptivePredict(self._input,60,self._ctx)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Select_list_elemContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def table_name(self):
            return self.getTypedRuleContext(SparksqlParser.Table_nameContext,0)


        def IDENTITY(self):
            return self.getToken(SparksqlParser.IDENTITY, 0)

        def ROWGUID(self):
            return self.getToken(SparksqlParser.ROWGUID, 0)

        def column_alias(self):
            return self.getTypedRuleContext(SparksqlParser.Column_aliasContext,0)


        def expression(self):
            return self.getTypedRuleContext(SparksqlParser.ExpressionContext,0)


        def AS(self):
            return self.getToken(SparksqlParser.AS, 0)

        def getRuleIndex(self):
            return SparksqlParser.RULE_select_list_elem

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSelect_list_elem" ):
                listener.enterSelect_list_elem(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSelect_list_elem" ):
                listener.exitSelect_list_elem(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSelect_list_elem" ):
                return visitor.visitSelect_list_elem(self)
            else:
                return visitor.visitChildren(self)




    def select_list_elem(self):

        localctx = SparksqlParser.Select_list_elemContext(self, self._ctx, self.state)
        self.enterRule(localctx, 66, self.RULE_select_list_elem)
        self._la = 0 # Token type
        try:
            self.state = 497
            la_ = self._interp.adaptivePredict(self._input,65,self._ctx)
            if la_ == 1:
                self.enterOuterAlt(localctx, 1)
                self.state = 479
                _la = self._input.LA(1)
                if ((((_la - 187)) & ~0x3f) == 0 and ((1 << (_la - 187)) & ((1 << (SparksqlParser.ABSOLUTE - 187)) | (1 << (SparksqlParser.APPLY - 187)) | (1 << (SparksqlParser.AUTO - 187)) | (1 << (SparksqlParser.AVG - 187)) | (1 << (SparksqlParser.BASE64 - 187)) | (1 << (SparksqlParser.CALLER - 187)) | (1 << (SparksqlParser.CAST - 187)) | (1 << (SparksqlParser.CATCH - 187)) | (1 << (SparksqlParser.CHECKSUM_AGG - 187)) | (1 << (SparksqlParser.COMMITTED - 187)) | (1 << (SparksqlParser.CONCAT - 187)) | (1 << (SparksqlParser.COOKIE - 187)) | (1 << (SparksqlParser.COUNT - 187)) | (1 << (SparksqlParser.COUNT_BIG - 187)) | (1 << (SparksqlParser.DELAY - 187)) | (1 << (SparksqlParser.DELETED - 187)) | (1 << (SparksqlParser.DENSE_RANK - 187)) | (1 << (SparksqlParser.DISABLE - 187)) | (1 << (SparksqlParser.DYNAMIC - 187)) | (1 << (SparksqlParser.ENCRYPTION - 187)) | (1 << (SparksqlParser.FAST - 187)) | (1 << (SparksqlParser.FAST_FORWARD - 187)) | (1 << (SparksqlParser.FIRST - 187)) | (1 << (SparksqlParser.FOLLOWING - 187)) | (1 << (SparksqlParser.FORWARD_ONLY - 187)) | (1 << (SparksqlParser.FULLSCAN - 187)) | (1 << (SparksqlParser.GLOBAL - 187)) | (1 << (SparksqlParser.GO - 187)) | (1 << (SparksqlParser.GROUPING - 187)) | (1 << (SparksqlParser.GROUPING_ID - 187)) | (1 << (SparksqlParser.HASH - 187)) | (1 << (SparksqlParser.INSENSITIVE - 187)) | (1 << (SparksqlParser.INSERTED - 187)) | (1 << (SparksqlParser.ISOLATION - 187)) | (1 << (SparksqlParser.KEEPFIXED - 187)) | (1 << (SparksqlParser.KEYSET - 187)) | (1 << (SparksqlParser.LAST - 187)) | (1 << (SparksqlParser.LEVEL - 187)) | (1 << (SparksqlParser.LOCAL - 187)) | (1 << (SparksqlParser.LOCK_ESCALATION - 187)) | (1 << (SparksqlParser.LOGIN - 187)) | (1 << (SparksqlParser.LOOP - 187)) | (1 << (SparksqlParser.MARK - 187)) | (1 << (SparksqlParser.MAX - 187)) | (1 << (SparksqlParser.MIN - 187)) | (1 << (SparksqlParser.MODIFY - 187)) | (1 << (SparksqlParser.NEXT - 187)) | (1 << (SparksqlParser.NAME - 187)) | (1 << (SparksqlParser.NOCOUNT - 187)) | (1 << (SparksqlParser.NOEXPAND - 187)) | (1 << (SparksqlParser.NORECOMPUTE - 187)) | (1 << (SparksqlParser.NTILE - 187)) | (1 << (SparksqlParser.NUMBER - 187)) | (1 << (SparksqlParser.OFFSET - 187)) | (1 << (SparksqlParser.ONLY - 187)) | (1 << (SparksqlParser.OPTIMISTIC - 187)) | (1 << (SparksqlParser.OPTIMIZE - 187)) | (1 << (SparksqlParser.OUT - 187)) | (1 << (SparksqlParser.OUTPUT - 187)) | (1 << (SparksqlParser.OWNER - 187)) | (1 << (SparksqlParser.PARTITION - 187)) | (1 << (SparksqlParser.PATH - 187)) | (1 << (SparksqlParser.PRECEDING - 187)) | (1 << (SparksqlParser.PRIOR - 187)))) != 0) or ((((_la - 251)) & ~0x3f) == 0 and ((1 << (_la - 251)) & ((1 << (SparksqlParser.RANGE - 251)) | (1 << (SparksqlParser.RANK - 251)) | (1 << (SparksqlParser.READONLY - 251)) | (1 << (SparksqlParser.READ_ONLY - 251)) | (1 << (SparksqlParser.RECOMPILE - 251)) | (1 << (SparksqlParser.RELATIVE - 251)) | (1 << (SparksqlParser.REMOTE - 251)) | (1 << (SparksqlParser.REPEATABLE - 251)) | (1 << (SparksqlParser.ROOT - 251)) | (1 << (SparksqlParser.ROW - 251)) | (1 << (SparksqlParser.ROWGUID - 251)) | (1 << (SparksqlParser.ROWS - 251)) | (1 << (SparksqlParser.ROW_NUMBER - 251)) | (1 << (SparksqlParser.SAMPLE - 251)) | (1 << (SparksqlParser.SCHEMABINDING - 251)) | (1 << (SparksqlParser.SCROLL - 251)) | (1 << (SparksqlParser.SCROLL_LOCKS - 251)) | (1 << (SparksqlParser.SELF - 251)) | (1 << (SparksqlParser.SERIALIZABLE - 251)) | (1 << (SparksqlParser.SNAPSHOT - 251)) | (1 << (SparksqlParser.STATIC - 251)) | (1 << (SparksqlParser.STATS_STREAM - 251)) | (1 << (SparksqlParser.STDEV - 251)) | (1 << (SparksqlParser.STDEVP - 251)) | (1 << (SparksqlParser.SUM - 251)) | (1 << (SparksqlParser.THROW - 251)) | (1 << (SparksqlParser.TIES - 251)) | (1 << (SparksqlParser.TIME - 251)) | (1 << (SparksqlParser.TRY - 251)) | (1 << (SparksqlParser.TYPE - 251)) | (1 << (SparksqlParser.TYPE_WARNING - 251)) | (1 << (SparksqlParser.UNBOUNDED - 251)) | (1 << (SparksqlParser.UNCOMMITTED - 251)) | (1 << (SparksqlParser.UNKNOWN - 251)) | (1 << (SparksqlParser.USING - 251)) | (1 << (SparksqlParser.VAR - 251)) | (1 << (SparksqlParser.VARP - 251)) | (1 << (SparksqlParser.VIEW_METADATA - 251)) | (1 << (SparksqlParser.WORK - 251)) | (1 << (SparksqlParser.XML - 251)) | (1 << (SparksqlParser.XMLNAMESPACES - 251)) | (1 << (SparksqlParser.DOUBLE_QUOTE_ID - 251)) | (1 << (SparksqlParser.SQUARE_BRACKET_ID - 251)) | (1 << (SparksqlParser.ID - 251)))) != 0) or _la==SparksqlParser.DOT:
                    self.state = 476
                    self.table_name()
                    self.state = 477
                    self.match(SparksqlParser.DOT)


                self.state = 484
                token = self._input.LA(1)
                if token in [SparksqlParser.STAR]:
                    self.state = 481
                    self.match(SparksqlParser.STAR)

                elif token in [SparksqlParser.DOLLAR]:
                    self.state = 482
                    self.match(SparksqlParser.DOLLAR)
                    self.state = 483
                    _la = self._input.LA(1)
                    if not(_la==SparksqlParser.IDENTITY or _la==SparksqlParser.ROWGUID):
                        self._errHandler.recoverInline(self)
                    else:
                        self.consume()

                else:
                    raise NoViableAltException(self)

                pass

            elif la_ == 2:
                self.enterOuterAlt(localctx, 2)
                self.state = 486
                self.column_alias()
                self.state = 487
                self.match(SparksqlParser.EQUAL)
                self.state = 488
                self.expression(0)
                pass

            elif la_ == 3:
                self.enterOuterAlt(localctx, 3)
                self.state = 490
                self.expression(0)
                self.state = 495
                la_ = self._interp.adaptivePredict(self._input,64,self._ctx)
                if la_ == 1:
                    self.state = 492
                    _la = self._input.LA(1)
                    if _la==SparksqlParser.AS:
                        self.state = 491
                        self.match(SparksqlParser.AS)


                    self.state = 494
                    self.column_alias()


                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Full_column_nameContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def column_name(self):
            return self.getTypedRuleContext(SparksqlParser.Column_nameContext,0)


        def table_name(self):
            return self.getTypedRuleContext(SparksqlParser.Table_nameContext,0)


        def getRuleIndex(self):
            return SparksqlParser.RULE_full_column_name

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterFull_column_name" ):
                listener.enterFull_column_name(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitFull_column_name" ):
                listener.exitFull_column_name(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitFull_column_name" ):
                return visitor.visitFull_column_name(self)
            else:
                return visitor.visitChildren(self)




    def full_column_name(self):

        localctx = SparksqlParser.Full_column_nameContext(self, self._ctx, self.state)
        self.enterRule(localctx, 68, self.RULE_full_column_name)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 502
            la_ = self._interp.adaptivePredict(self._input,66,self._ctx)
            if la_ == 1:
                self.state = 499
                self.table_name()
                self.state = 500
                self.match(SparksqlParser.DOT)


            self.state = 504
            self.column_name()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Column_nameContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def id_1(self):
            return self.getTypedRuleContext(SparksqlParser.Id_1Context,0)


        def getRuleIndex(self):
            return SparksqlParser.RULE_column_name

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterColumn_name" ):
                listener.enterColumn_name(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitColumn_name" ):
                listener.exitColumn_name(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitColumn_name" ):
                return visitor.visitColumn_name(self)
            else:
                return visitor.visitChildren(self)




    def column_name(self):

        localctx = SparksqlParser.Column_nameContext(self, self._ctx, self.state)
        self.enterRule(localctx, 70, self.RULE_column_name)
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 506
            self.id_1()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Case_exprContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def CASE(self):
            return self.getToken(SparksqlParser.CASE, 0)

        def expression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.ExpressionContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.ExpressionContext,i)


        def END(self):
            return self.getToken(SparksqlParser.END, 0)

        def WHEN(self, i:int=None):
            if i is None:
                return self.getTokens(SparksqlParser.WHEN)
            else:
                return self.getToken(SparksqlParser.WHEN, i)

        def THEN(self, i:int=None):
            if i is None:
                return self.getTokens(SparksqlParser.THEN)
            else:
                return self.getToken(SparksqlParser.THEN, i)

        def ELSE(self):
            return self.getToken(SparksqlParser.ELSE, 0)

        def search_condition(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.Search_conditionContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.Search_conditionContext,i)


        def getRuleIndex(self):
            return SparksqlParser.RULE_case_expr

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterCase_expr" ):
                listener.enterCase_expr(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitCase_expr" ):
                listener.exitCase_expr(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitCase_expr" ):
                return visitor.visitCase_expr(self)
            else:
                return visitor.visitChildren(self)




    def case_expr(self):

        localctx = SparksqlParser.Case_exprContext(self, self._ctx, self.state)
        self.enterRule(localctx, 72, self.RULE_case_expr)
        self._la = 0 # Token type
        try:
            self.state = 541
            la_ = self._interp.adaptivePredict(self._input,71,self._ctx)
            if la_ == 1:
                self.enterOuterAlt(localctx, 1)
                self.state = 508
                self.match(SparksqlParser.CASE)
                self.state = 509
                self.expression(0)
                self.state = 515 
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while True:
                    self.state = 510
                    self.match(SparksqlParser.WHEN)
                    self.state = 511
                    self.expression(0)
                    self.state = 512
                    self.match(SparksqlParser.THEN)
                    self.state = 513
                    self.expression(0)
                    self.state = 517 
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    if not (_la==SparksqlParser.WHEN):
                        break

                self.state = 521
                _la = self._input.LA(1)
                if _la==SparksqlParser.ELSE:
                    self.state = 519
                    self.match(SparksqlParser.ELSE)
                    self.state = 520
                    self.expression(0)


                self.state = 523
                self.match(SparksqlParser.END)
                pass

            elif la_ == 2:
                self.enterOuterAlt(localctx, 2)
                self.state = 525
                self.match(SparksqlParser.CASE)
                self.state = 531 
                self._errHandler.sync(self)
                _la = self._input.LA(1)
                while True:
                    self.state = 526
                    self.match(SparksqlParser.WHEN)
                    self.state = 527
                    self.search_condition()
                    self.state = 528
                    self.match(SparksqlParser.THEN)
                    self.state = 529
                    self.expression(0)
                    self.state = 533 
                    self._errHandler.sync(self)
                    _la = self._input.LA(1)
                    if not (_la==SparksqlParser.WHEN):
                        break

                self.state = 537
                _la = self._input.LA(1)
                if _la==SparksqlParser.ELSE:
                    self.state = 535
                    self.match(SparksqlParser.ELSE)
                    self.state = 536
                    self.expression(0)


                self.state = 539
                self.match(SparksqlParser.END)
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Table_nameContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser
            self.schema = None # Id_1Context
            self.table = None # Id_1Context

        def id_1(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.Id_1Context)
            else:
                return self.getTypedRuleContext(SparksqlParser.Id_1Context,i)


        def getRuleIndex(self):
            return SparksqlParser.RULE_table_name

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterTable_name" ):
                listener.enterTable_name(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitTable_name" ):
                listener.exitTable_name(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitTable_name" ):
                return visitor.visitTable_name(self)
            else:
                return visitor.visitChildren(self)




    def table_name(self):

        localctx = SparksqlParser.Table_nameContext(self, self._ctx, self.state)
        self.enterRule(localctx, 74, self.RULE_table_name)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 550
            la_ = self._interp.adaptivePredict(self._input,73,self._ctx)
            if la_ == 1:
                self.state = 544
                _la = self._input.LA(1)
                if ((((_la - 187)) & ~0x3f) == 0 and ((1 << (_la - 187)) & ((1 << (SparksqlParser.ABSOLUTE - 187)) | (1 << (SparksqlParser.APPLY - 187)) | (1 << (SparksqlParser.AUTO - 187)) | (1 << (SparksqlParser.AVG - 187)) | (1 << (SparksqlParser.BASE64 - 187)) | (1 << (SparksqlParser.CALLER - 187)) | (1 << (SparksqlParser.CAST - 187)) | (1 << (SparksqlParser.CATCH - 187)) | (1 << (SparksqlParser.CHECKSUM_AGG - 187)) | (1 << (SparksqlParser.COMMITTED - 187)) | (1 << (SparksqlParser.CONCAT - 187)) | (1 << (SparksqlParser.COOKIE - 187)) | (1 << (SparksqlParser.COUNT - 187)) | (1 << (SparksqlParser.COUNT_BIG - 187)) | (1 << (SparksqlParser.DELAY - 187)) | (1 << (SparksqlParser.DELETED - 187)) | (1 << (SparksqlParser.DENSE_RANK - 187)) | (1 << (SparksqlParser.DISABLE - 187)) | (1 << (SparksqlParser.DYNAMIC - 187)) | (1 << (SparksqlParser.ENCRYPTION - 187)) | (1 << (SparksqlParser.FAST - 187)) | (1 << (SparksqlParser.FAST_FORWARD - 187)) | (1 << (SparksqlParser.FIRST - 187)) | (1 << (SparksqlParser.FOLLOWING - 187)) | (1 << (SparksqlParser.FORWARD_ONLY - 187)) | (1 << (SparksqlParser.FULLSCAN - 187)) | (1 << (SparksqlParser.GLOBAL - 187)) | (1 << (SparksqlParser.GO - 187)) | (1 << (SparksqlParser.GROUPING - 187)) | (1 << (SparksqlParser.GROUPING_ID - 187)) | (1 << (SparksqlParser.HASH - 187)) | (1 << (SparksqlParser.INSENSITIVE - 187)) | (1 << (SparksqlParser.INSERTED - 187)) | (1 << (SparksqlParser.ISOLATION - 187)) | (1 << (SparksqlParser.KEEPFIXED - 187)) | (1 << (SparksqlParser.KEYSET - 187)) | (1 << (SparksqlParser.LAST - 187)) | (1 << (SparksqlParser.LEVEL - 187)) | (1 << (SparksqlParser.LOCAL - 187)) | (1 << (SparksqlParser.LOCK_ESCALATION - 187)) | (1 << (SparksqlParser.LOGIN - 187)) | (1 << (SparksqlParser.LOOP - 187)) | (1 << (SparksqlParser.MARK - 187)) | (1 << (SparksqlParser.MAX - 187)) | (1 << (SparksqlParser.MIN - 187)) | (1 << (SparksqlParser.MODIFY - 187)) | (1 << (SparksqlParser.NEXT - 187)) | (1 << (SparksqlParser.NAME - 187)) | (1 << (SparksqlParser.NOCOUNT - 187)) | (1 << (SparksqlParser.NOEXPAND - 187)) | (1 << (SparksqlParser.NORECOMPUTE - 187)) | (1 << (SparksqlParser.NTILE - 187)) | (1 << (SparksqlParser.NUMBER - 187)) | (1 << (SparksqlParser.OFFSET - 187)) | (1 << (SparksqlParser.ONLY - 187)) | (1 << (SparksqlParser.OPTIMISTIC - 187)) | (1 << (SparksqlParser.OPTIMIZE - 187)) | (1 << (SparksqlParser.OUT - 187)) | (1 << (SparksqlParser.OUTPUT - 187)) | (1 << (SparksqlParser.OWNER - 187)) | (1 << (SparksqlParser.PARTITION - 187)) | (1 << (SparksqlParser.PATH - 187)) | (1 << (SparksqlParser.PRECEDING - 187)) | (1 << (SparksqlParser.PRIOR - 187)))) != 0) or ((((_la - 251)) & ~0x3f) == 0 and ((1 << (_la - 251)) & ((1 << (SparksqlParser.RANGE - 251)) | (1 << (SparksqlParser.RANK - 251)) | (1 << (SparksqlParser.READONLY - 251)) | (1 << (SparksqlParser.READ_ONLY - 251)) | (1 << (SparksqlParser.RECOMPILE - 251)) | (1 << (SparksqlParser.RELATIVE - 251)) | (1 << (SparksqlParser.REMOTE - 251)) | (1 << (SparksqlParser.REPEATABLE - 251)) | (1 << (SparksqlParser.ROOT - 251)) | (1 << (SparksqlParser.ROW - 251)) | (1 << (SparksqlParser.ROWGUID - 251)) | (1 << (SparksqlParser.ROWS - 251)) | (1 << (SparksqlParser.ROW_NUMBER - 251)) | (1 << (SparksqlParser.SAMPLE - 251)) | (1 << (SparksqlParser.SCHEMABINDING - 251)) | (1 << (SparksqlParser.SCROLL - 251)) | (1 << (SparksqlParser.SCROLL_LOCKS - 251)) | (1 << (SparksqlParser.SELF - 251)) | (1 << (SparksqlParser.SERIALIZABLE - 251)) | (1 << (SparksqlParser.SNAPSHOT - 251)) | (1 << (SparksqlParser.STATIC - 251)) | (1 << (SparksqlParser.STATS_STREAM - 251)) | (1 << (SparksqlParser.STDEV - 251)) | (1 << (SparksqlParser.STDEVP - 251)) | (1 << (SparksqlParser.SUM - 251)) | (1 << (SparksqlParser.THROW - 251)) | (1 << (SparksqlParser.TIES - 251)) | (1 << (SparksqlParser.TIME - 251)) | (1 << (SparksqlParser.TRY - 251)) | (1 << (SparksqlParser.TYPE - 251)) | (1 << (SparksqlParser.TYPE_WARNING - 251)) | (1 << (SparksqlParser.UNBOUNDED - 251)) | (1 << (SparksqlParser.UNCOMMITTED - 251)) | (1 << (SparksqlParser.UNKNOWN - 251)) | (1 << (SparksqlParser.USING - 251)) | (1 << (SparksqlParser.VAR - 251)) | (1 << (SparksqlParser.VARP - 251)) | (1 << (SparksqlParser.VIEW_METADATA - 251)) | (1 << (SparksqlParser.WORK - 251)) | (1 << (SparksqlParser.XML - 251)) | (1 << (SparksqlParser.XMLNAMESPACES - 251)) | (1 << (SparksqlParser.DOUBLE_QUOTE_ID - 251)) | (1 << (SparksqlParser.SQUARE_BRACKET_ID - 251)) | (1 << (SparksqlParser.ID - 251)))) != 0):
                    self.state = 543
                    localctx.schema = self.id_1()


                self.state = 546
                self.match(SparksqlParser.DOT)

            elif la_ == 2:
                self.state = 547
                localctx.schema = self.id_1()
                self.state = 548
                self.match(SparksqlParser.DOT)


            self.state = 552
            localctx.table = self.id_1()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Search_conditionContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def search_condition_or(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.Search_condition_orContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.Search_condition_orContext,i)


        def AND(self, i:int=None):
            if i is None:
                return self.getTokens(SparksqlParser.AND)
            else:
                return self.getToken(SparksqlParser.AND, i)

        def getRuleIndex(self):
            return SparksqlParser.RULE_search_condition

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSearch_condition" ):
                listener.enterSearch_condition(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSearch_condition" ):
                listener.exitSearch_condition(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSearch_condition" ):
                return visitor.visitSearch_condition(self)
            else:
                return visitor.visitChildren(self)




    def search_condition(self):

        localctx = SparksqlParser.Search_conditionContext(self, self._ctx, self.state)
        self.enterRule(localctx, 76, self.RULE_search_condition)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 554
            self.search_condition_or()
            self.state = 559
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparksqlParser.AND:
                self.state = 555
                self.match(SparksqlParser.AND)
                self.state = 556
                self.search_condition_or()
                self.state = 561
                self._errHandler.sync(self)
                _la = self._input.LA(1)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Search_condition_orContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def search_condition_not(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.Search_condition_notContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.Search_condition_notContext,i)


        def OR(self, i:int=None):
            if i is None:
                return self.getTokens(SparksqlParser.OR)
            else:
                return self.getToken(SparksqlParser.OR, i)

        def getRuleIndex(self):
            return SparksqlParser.RULE_search_condition_or

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSearch_condition_or" ):
                listener.enterSearch_condition_or(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSearch_condition_or" ):
                listener.exitSearch_condition_or(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSearch_condition_or" ):
                return visitor.visitSearch_condition_or(self)
            else:
                return visitor.visitChildren(self)




    def search_condition_or(self):

        localctx = SparksqlParser.Search_condition_orContext(self, self._ctx, self.state)
        self.enterRule(localctx, 78, self.RULE_search_condition_or)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 562
            self.search_condition_not()
            self.state = 567
            self._errHandler.sync(self)
            _la = self._input.LA(1)
            while _la==SparksqlParser.OR:
                self.state = 563
                self.match(SparksqlParser.OR)
                self.state = 564
                self.search_condition_not()
                self.state = 569
                self._errHandler.sync(self)
                _la = self._input.LA(1)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Search_condition_notContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def predicate(self):
            return self.getTypedRuleContext(SparksqlParser.PredicateContext,0)


        def NOT(self):
            return self.getToken(SparksqlParser.NOT, 0)

        def getRuleIndex(self):
            return SparksqlParser.RULE_search_condition_not

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSearch_condition_not" ):
                listener.enterSearch_condition_not(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSearch_condition_not" ):
                listener.exitSearch_condition_not(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSearch_condition_not" ):
                return visitor.visitSearch_condition_not(self)
            else:
                return visitor.visitChildren(self)




    def search_condition_not(self):

        localctx = SparksqlParser.Search_condition_notContext(self, self._ctx, self.state)
        self.enterRule(localctx, 80, self.RULE_search_condition_not)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 571
            _la = self._input.LA(1)
            if _la==SparksqlParser.NOT:
                self.state = 570
                self.match(SparksqlParser.NOT)


            self.state = 573
            self.predicate()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class PredicateContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def EXISTS(self):
            return self.getToken(SparksqlParser.EXISTS, 0)

        def subquery(self):
            return self.getTypedRuleContext(SparksqlParser.SubqueryContext,0)


        def expression(self, i:int=None):
            if i is None:
                return self.getTypedRuleContexts(SparksqlParser.ExpressionContext)
            else:
                return self.getTypedRuleContext(SparksqlParser.ExpressionContext,i)


        def comparison_operator(self):
            return self.getTypedRuleContext(SparksqlParser.Comparison_operatorContext,0)


        def ALL(self):
            return self.getToken(SparksqlParser.ALL, 0)

        def SOME(self):
            return self.getToken(SparksqlParser.SOME, 0)

        def ANY(self):
            return self.getToken(SparksqlParser.ANY, 0)

        def BETWEEN(self):
            return self.getToken(SparksqlParser.BETWEEN, 0)

        def AND(self):
            return self.getToken(SparksqlParser.AND, 0)

        def NOT(self):
            return self.getToken(SparksqlParser.NOT, 0)

        def IN(self):
            return self.getToken(SparksqlParser.IN, 0)

        def expression_list(self):
            return self.getTypedRuleContext(SparksqlParser.Expression_listContext,0)


        def LIKE(self):
            return self.getToken(SparksqlParser.LIKE, 0)

        def ESCAPE(self):
            return self.getToken(SparksqlParser.ESCAPE, 0)

        def IS(self):
            return self.getToken(SparksqlParser.IS, 0)

        def null_notnull(self):
            return self.getTypedRuleContext(SparksqlParser.Null_notnullContext,0)


        def search_condition(self):
            return self.getTypedRuleContext(SparksqlParser.Search_conditionContext,0)


        def getRuleIndex(self):
            return SparksqlParser.RULE_predicate

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterPredicate" ):
                listener.enterPredicate(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitPredicate" ):
                listener.exitPredicate(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitPredicate" ):
                return visitor.visitPredicate(self)
            else:
                return visitor.visitChildren(self)




    def predicate(self):

        localctx = SparksqlParser.PredicateContext(self, self._ctx, self.state)
        self.enterRule(localctx, 82, self.RULE_predicate)
        self._la = 0 # Token type
        try:
            self.state = 630
            la_ = self._interp.adaptivePredict(self._input,82,self._ctx)
            if la_ == 1:
                self.enterOuterAlt(localctx, 1)
                self.state = 575
                self.match(SparksqlParser.EXISTS)
                self.state = 576
                self.match(SparksqlParser.LR_BRACKET)
                self.state = 577
                self.subquery()
                self.state = 578
                self.match(SparksqlParser.RR_BRACKET)
                pass

            elif la_ == 2:
                self.enterOuterAlt(localctx, 2)
                self.state = 580
                self.expression(0)
                self.state = 581
                self.comparison_operator()
                self.state = 582
                self.expression(0)
                pass

            elif la_ == 3:
                self.enterOuterAlt(localctx, 3)
                self.state = 584
                self.expression(0)
                self.state = 585
                self.comparison_operator()
                self.state = 586
                _la = self._input.LA(1)
                if not(_la==SparksqlParser.ALL or _la==SparksqlParser.ANY or _la==SparksqlParser.SOME):
                    self._errHandler.recoverInline(self)
                else:
                    self.consume()
                self.state = 587
                self.match(SparksqlParser.LR_BRACKET)
                self.state = 588
                self.subquery()
                self.state = 589
                self.match(SparksqlParser.RR_BRACKET)
                pass

            elif la_ == 4:
                self.enterOuterAlt(localctx, 4)
                self.state = 591
                self.expression(0)
                self.state = 593
                _la = self._input.LA(1)
                if _la==SparksqlParser.NOT:
                    self.state = 592
                    self.match(SparksqlParser.NOT)


                self.state = 595
                self.match(SparksqlParser.BETWEEN)
                self.state = 596
                self.expression(0)
                self.state = 597
                self.match(SparksqlParser.AND)
                self.state = 598
                self.expression(0)
                pass

            elif la_ == 5:
                self.enterOuterAlt(localctx, 5)
                self.state = 600
                self.expression(0)
                self.state = 602
                _la = self._input.LA(1)
                if _la==SparksqlParser.NOT:
                    self.state = 601
                    self.match(SparksqlParser.NOT)


                self.state = 604
                self.match(SparksqlParser.IN)
                self.state = 605
                self.match(SparksqlParser.LR_BRACKET)
                self.state = 608
                la_ = self._interp.adaptivePredict(self._input,79,self._ctx)
                if la_ == 1:
                    self.state = 606
                    self.subquery()
                    pass

                elif la_ == 2:
                    self.state = 607
                    self.expression_list()
                    pass


                self.state = 610
                self.match(SparksqlParser.RR_BRACKET)
                pass

            elif la_ == 6:
                self.enterOuterAlt(localctx, 6)
                self.state = 612
                self.expression(0)
                self.state = 614
                _la = self._input.LA(1)
                if _la==SparksqlParser.NOT:
                    self.state = 613
                    self.match(SparksqlParser.NOT)


                self.state = 616
                self.match(SparksqlParser.LIKE)
                self.state = 617
                self.expression(0)
                self.state = 620
                _la = self._input.LA(1)
                if _la==SparksqlParser.ESCAPE:
                    self.state = 618
                    self.match(SparksqlParser.ESCAPE)
                    self.state = 619
                    self.expression(0)


                pass

            elif la_ == 7:
                self.enterOuterAlt(localctx, 7)
                self.state = 622
                self.expression(0)
                self.state = 623
                self.match(SparksqlParser.IS)
                self.state = 624
                self.null_notnull()
                pass

            elif la_ == 8:
                self.enterOuterAlt(localctx, 8)
                self.state = 626
                self.match(SparksqlParser.LR_BRACKET)
                self.state = 627
                self.search_condition()
                self.state = 628
                self.match(SparksqlParser.RR_BRACKET)
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Id_1Context(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def simple_id(self):
            return self.getTypedRuleContext(SparksqlParser.Simple_idContext,0)


        def DOUBLE_QUOTE_ID(self):
            return self.getToken(SparksqlParser.DOUBLE_QUOTE_ID, 0)

        def SQUARE_BRACKET_ID(self):
            return self.getToken(SparksqlParser.SQUARE_BRACKET_ID, 0)

        def getRuleIndex(self):
            return SparksqlParser.RULE_id_1

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterId_1" ):
                listener.enterId_1(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitId_1" ):
                listener.exitId_1(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitId_1" ):
                return visitor.visitId_1(self)
            else:
                return visitor.visitChildren(self)




    def id_1(self):

        localctx = SparksqlParser.Id_1Context(self, self._ctx, self.state)
        self.enterRule(localctx, 84, self.RULE_id_1)
        try:
            self.state = 635
            token = self._input.LA(1)
            if token in [SparksqlParser.ABSOLUTE, SparksqlParser.APPLY, SparksqlParser.AUTO, SparksqlParser.AVG, SparksqlParser.BASE64, SparksqlParser.CALLER, SparksqlParser.CAST, SparksqlParser.CATCH, SparksqlParser.CHECKSUM_AGG, SparksqlParser.COMMITTED, SparksqlParser.CONCAT, SparksqlParser.COOKIE, SparksqlParser.COUNT, SparksqlParser.COUNT_BIG, SparksqlParser.DELAY, SparksqlParser.DELETED, SparksqlParser.DENSE_RANK, SparksqlParser.DISABLE, SparksqlParser.DYNAMIC, SparksqlParser.ENCRYPTION, SparksqlParser.FAST, SparksqlParser.FAST_FORWARD, SparksqlParser.FIRST, SparksqlParser.FOLLOWING, SparksqlParser.FORWARD_ONLY, SparksqlParser.FULLSCAN, SparksqlParser.GLOBAL, SparksqlParser.GO, SparksqlParser.GROUPING, SparksqlParser.GROUPING_ID, SparksqlParser.HASH, SparksqlParser.INSENSITIVE, SparksqlParser.INSERTED, SparksqlParser.ISOLATION, SparksqlParser.KEEPFIXED, SparksqlParser.KEYSET, SparksqlParser.LAST, SparksqlParser.LEVEL, SparksqlParser.LOCAL, SparksqlParser.LOCK_ESCALATION, SparksqlParser.LOGIN, SparksqlParser.LOOP, SparksqlParser.MARK, SparksqlParser.MAX, SparksqlParser.MIN, SparksqlParser.MODIFY, SparksqlParser.NEXT, SparksqlParser.NAME, SparksqlParser.NOCOUNT, SparksqlParser.NOEXPAND, SparksqlParser.NORECOMPUTE, SparksqlParser.NTILE, SparksqlParser.NUMBER, SparksqlParser.OFFSET, SparksqlParser.ONLY, SparksqlParser.OPTIMISTIC, SparksqlParser.OPTIMIZE, SparksqlParser.OUT, SparksqlParser.OUTPUT, SparksqlParser.OWNER, SparksqlParser.PARTITION, SparksqlParser.PATH, SparksqlParser.PRECEDING, SparksqlParser.PRIOR, SparksqlParser.RANGE, SparksqlParser.RANK, SparksqlParser.READONLY, SparksqlParser.READ_ONLY, SparksqlParser.RECOMPILE, SparksqlParser.RELATIVE, SparksqlParser.REMOTE, SparksqlParser.REPEATABLE, SparksqlParser.ROOT, SparksqlParser.ROW, SparksqlParser.ROWGUID, SparksqlParser.ROWS, SparksqlParser.ROW_NUMBER, SparksqlParser.SAMPLE, SparksqlParser.SCHEMABINDING, SparksqlParser.SCROLL, SparksqlParser.SCROLL_LOCKS, SparksqlParser.SELF, SparksqlParser.SERIALIZABLE, SparksqlParser.SNAPSHOT, SparksqlParser.STATIC, SparksqlParser.STATS_STREAM, SparksqlParser.STDEV, SparksqlParser.STDEVP, SparksqlParser.SUM, SparksqlParser.THROW, SparksqlParser.TIES, SparksqlParser.TIME, SparksqlParser.TRY, SparksqlParser.TYPE, SparksqlParser.TYPE_WARNING, SparksqlParser.UNBOUNDED, SparksqlParser.UNCOMMITTED, SparksqlParser.UNKNOWN, SparksqlParser.USING, SparksqlParser.VAR, SparksqlParser.VARP, SparksqlParser.VIEW_METADATA, SparksqlParser.WORK, SparksqlParser.XML, SparksqlParser.XMLNAMESPACES, SparksqlParser.ID]:
                self.enterOuterAlt(localctx, 1)
                self.state = 632
                self.simple_id()

            elif token in [SparksqlParser.DOUBLE_QUOTE_ID]:
                self.enterOuterAlt(localctx, 2)
                self.state = 633
                self.match(SparksqlParser.DOUBLE_QUOTE_ID)

            elif token in [SparksqlParser.SQUARE_BRACKET_ID]:
                self.enterOuterAlt(localctx, 3)
                self.state = 634
                self.match(SparksqlParser.SQUARE_BRACKET_ID)

            else:
                raise NoViableAltException(self)

        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Comparison_operatorContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser


        def getRuleIndex(self):
            return SparksqlParser.RULE_comparison_operator

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterComparison_operator" ):
                listener.enterComparison_operator(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitComparison_operator" ):
                listener.exitComparison_operator(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitComparison_operator" ):
                return visitor.visitComparison_operator(self)
            else:
                return visitor.visitChildren(self)




    def comparison_operator(self):

        localctx = SparksqlParser.Comparison_operatorContext(self, self._ctx, self.state)
        self.enterRule(localctx, 86, self.RULE_comparison_operator)
        try:
            self.state = 652
            la_ = self._interp.adaptivePredict(self._input,84,self._ctx)
            if la_ == 1:
                self.enterOuterAlt(localctx, 1)
                self.state = 637
                self.match(SparksqlParser.EQUAL)
                pass

            elif la_ == 2:
                self.enterOuterAlt(localctx, 2)
                self.state = 638
                self.match(SparksqlParser.GREATER)
                pass

            elif la_ == 3:
                self.enterOuterAlt(localctx, 3)
                self.state = 639
                self.match(SparksqlParser.LESS)
                pass

            elif la_ == 4:
                self.enterOuterAlt(localctx, 4)
                self.state = 640
                self.match(SparksqlParser.LESS)
                self.state = 641
                self.match(SparksqlParser.EQUAL)
                pass

            elif la_ == 5:
                self.enterOuterAlt(localctx, 5)
                self.state = 642
                self.match(SparksqlParser.GREATER)
                self.state = 643
                self.match(SparksqlParser.EQUAL)
                pass

            elif la_ == 6:
                self.enterOuterAlt(localctx, 6)
                self.state = 644
                self.match(SparksqlParser.LESS)
                self.state = 645
                self.match(SparksqlParser.GREATER)
                pass

            elif la_ == 7:
                self.enterOuterAlt(localctx, 7)
                self.state = 646
                self.match(SparksqlParser.EXCLAMATION)
                self.state = 647
                self.match(SparksqlParser.EQUAL)
                pass

            elif la_ == 8:
                self.enterOuterAlt(localctx, 8)
                self.state = 648
                self.match(SparksqlParser.EXCLAMATION)
                self.state = 649
                self.match(SparksqlParser.GREATER)
                pass

            elif la_ == 9:
                self.enterOuterAlt(localctx, 9)
                self.state = 650
                self.match(SparksqlParser.EXCLAMATION)
                self.state = 651
                self.match(SparksqlParser.LESS)
                pass


        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Simple_idContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def ID(self):
            return self.getToken(SparksqlParser.ID, 0)

        def ABSOLUTE(self):
            return self.getToken(SparksqlParser.ABSOLUTE, 0)

        def APPLY(self):
            return self.getToken(SparksqlParser.APPLY, 0)

        def AUTO(self):
            return self.getToken(SparksqlParser.AUTO, 0)

        def AVG(self):
            return self.getToken(SparksqlParser.AVG, 0)

        def BASE64(self):
            return self.getToken(SparksqlParser.BASE64, 0)

        def CALLER(self):
            return self.getToken(SparksqlParser.CALLER, 0)

        def CAST(self):
            return self.getToken(SparksqlParser.CAST, 0)

        def CATCH(self):
            return self.getToken(SparksqlParser.CATCH, 0)

        def CHECKSUM_AGG(self):
            return self.getToken(SparksqlParser.CHECKSUM_AGG, 0)

        def COMMITTED(self):
            return self.getToken(SparksqlParser.COMMITTED, 0)

        def CONCAT(self):
            return self.getToken(SparksqlParser.CONCAT, 0)

        def COOKIE(self):
            return self.getToken(SparksqlParser.COOKIE, 0)

        def COUNT(self):
            return self.getToken(SparksqlParser.COUNT, 0)

        def COUNT_BIG(self):
            return self.getToken(SparksqlParser.COUNT_BIG, 0)

        def DELAY(self):
            return self.getToken(SparksqlParser.DELAY, 0)

        def DELETED(self):
            return self.getToken(SparksqlParser.DELETED, 0)

        def DENSE_RANK(self):
            return self.getToken(SparksqlParser.DENSE_RANK, 0)

        def DISABLE(self):
            return self.getToken(SparksqlParser.DISABLE, 0)

        def DYNAMIC(self):
            return self.getToken(SparksqlParser.DYNAMIC, 0)

        def ENCRYPTION(self):
            return self.getToken(SparksqlParser.ENCRYPTION, 0)

        def FAST(self):
            return self.getToken(SparksqlParser.FAST, 0)

        def FAST_FORWARD(self):
            return self.getToken(SparksqlParser.FAST_FORWARD, 0)

        def FIRST(self):
            return self.getToken(SparksqlParser.FIRST, 0)

        def FOLLOWING(self):
            return self.getToken(SparksqlParser.FOLLOWING, 0)

        def FORWARD_ONLY(self):
            return self.getToken(SparksqlParser.FORWARD_ONLY, 0)

        def FULLSCAN(self):
            return self.getToken(SparksqlParser.FULLSCAN, 0)

        def GLOBAL(self):
            return self.getToken(SparksqlParser.GLOBAL, 0)

        def GO(self):
            return self.getToken(SparksqlParser.GO, 0)

        def GROUPING(self):
            return self.getToken(SparksqlParser.GROUPING, 0)

        def GROUPING_ID(self):
            return self.getToken(SparksqlParser.GROUPING_ID, 0)

        def HASH(self):
            return self.getToken(SparksqlParser.HASH, 0)

        def INSENSITIVE(self):
            return self.getToken(SparksqlParser.INSENSITIVE, 0)

        def INSERTED(self):
            return self.getToken(SparksqlParser.INSERTED, 0)

        def ISOLATION(self):
            return self.getToken(SparksqlParser.ISOLATION, 0)

        def KEYSET(self):
            return self.getToken(SparksqlParser.KEYSET, 0)

        def KEEPFIXED(self):
            return self.getToken(SparksqlParser.KEEPFIXED, 0)

        def LAST(self):
            return self.getToken(SparksqlParser.LAST, 0)

        def LEVEL(self):
            return self.getToken(SparksqlParser.LEVEL, 0)

        def LOCAL(self):
            return self.getToken(SparksqlParser.LOCAL, 0)

        def LOCK_ESCALATION(self):
            return self.getToken(SparksqlParser.LOCK_ESCALATION, 0)

        def LOGIN(self):
            return self.getToken(SparksqlParser.LOGIN, 0)

        def LOOP(self):
            return self.getToken(SparksqlParser.LOOP, 0)

        def MARK(self):
            return self.getToken(SparksqlParser.MARK, 0)

        def MAX(self):
            return self.getToken(SparksqlParser.MAX, 0)

        def MIN(self):
            return self.getToken(SparksqlParser.MIN, 0)

        def MODIFY(self):
            return self.getToken(SparksqlParser.MODIFY, 0)

        def NAME(self):
            return self.getToken(SparksqlParser.NAME, 0)

        def NEXT(self):
            return self.getToken(SparksqlParser.NEXT, 0)

        def NOCOUNT(self):
            return self.getToken(SparksqlParser.NOCOUNT, 0)

        def NOEXPAND(self):
            return self.getToken(SparksqlParser.NOEXPAND, 0)

        def NORECOMPUTE(self):
            return self.getToken(SparksqlParser.NORECOMPUTE, 0)

        def NTILE(self):
            return self.getToken(SparksqlParser.NTILE, 0)

        def NUMBER(self):
            return self.getToken(SparksqlParser.NUMBER, 0)

        def OFFSET(self):
            return self.getToken(SparksqlParser.OFFSET, 0)

        def ONLY(self):
            return self.getToken(SparksqlParser.ONLY, 0)

        def OPTIMISTIC(self):
            return self.getToken(SparksqlParser.OPTIMISTIC, 0)

        def OPTIMIZE(self):
            return self.getToken(SparksqlParser.OPTIMIZE, 0)

        def OUT(self):
            return self.getToken(SparksqlParser.OUT, 0)

        def OUTPUT(self):
            return self.getToken(SparksqlParser.OUTPUT, 0)

        def OWNER(self):
            return self.getToken(SparksqlParser.OWNER, 0)

        def PARTITION(self):
            return self.getToken(SparksqlParser.PARTITION, 0)

        def PATH(self):
            return self.getToken(SparksqlParser.PATH, 0)

        def PRECEDING(self):
            return self.getToken(SparksqlParser.PRECEDING, 0)

        def PRIOR(self):
            return self.getToken(SparksqlParser.PRIOR, 0)

        def RANGE(self):
            return self.getToken(SparksqlParser.RANGE, 0)

        def RANK(self):
            return self.getToken(SparksqlParser.RANK, 0)

        def READONLY(self):
            return self.getToken(SparksqlParser.READONLY, 0)

        def READ_ONLY(self):
            return self.getToken(SparksqlParser.READ_ONLY, 0)

        def RECOMPILE(self):
            return self.getToken(SparksqlParser.RECOMPILE, 0)

        def RELATIVE(self):
            return self.getToken(SparksqlParser.RELATIVE, 0)

        def REMOTE(self):
            return self.getToken(SparksqlParser.REMOTE, 0)

        def REPEATABLE(self):
            return self.getToken(SparksqlParser.REPEATABLE, 0)

        def ROOT(self):
            return self.getToken(SparksqlParser.ROOT, 0)

        def ROW(self):
            return self.getToken(SparksqlParser.ROW, 0)

        def ROWGUID(self):
            return self.getToken(SparksqlParser.ROWGUID, 0)

        def ROWS(self):
            return self.getToken(SparksqlParser.ROWS, 0)

        def ROW_NUMBER(self):
            return self.getToken(SparksqlParser.ROW_NUMBER, 0)

        def SAMPLE(self):
            return self.getToken(SparksqlParser.SAMPLE, 0)

        def SCHEMABINDING(self):
            return self.getToken(SparksqlParser.SCHEMABINDING, 0)

        def SCROLL(self):
            return self.getToken(SparksqlParser.SCROLL, 0)

        def SCROLL_LOCKS(self):
            return self.getToken(SparksqlParser.SCROLL_LOCKS, 0)

        def SELF(self):
            return self.getToken(SparksqlParser.SELF, 0)

        def SERIALIZABLE(self):
            return self.getToken(SparksqlParser.SERIALIZABLE, 0)

        def SNAPSHOT(self):
            return self.getToken(SparksqlParser.SNAPSHOT, 0)

        def STATIC(self):
            return self.getToken(SparksqlParser.STATIC, 0)

        def STATS_STREAM(self):
            return self.getToken(SparksqlParser.STATS_STREAM, 0)

        def STDEV(self):
            return self.getToken(SparksqlParser.STDEV, 0)

        def STDEVP(self):
            return self.getToken(SparksqlParser.STDEVP, 0)

        def SUM(self):
            return self.getToken(SparksqlParser.SUM, 0)

        def THROW(self):
            return self.getToken(SparksqlParser.THROW, 0)

        def TIES(self):
            return self.getToken(SparksqlParser.TIES, 0)

        def TIME(self):
            return self.getToken(SparksqlParser.TIME, 0)

        def TRY(self):
            return self.getToken(SparksqlParser.TRY, 0)

        def TYPE(self):
            return self.getToken(SparksqlParser.TYPE, 0)

        def TYPE_WARNING(self):
            return self.getToken(SparksqlParser.TYPE_WARNING, 0)

        def UNBOUNDED(self):
            return self.getToken(SparksqlParser.UNBOUNDED, 0)

        def UNCOMMITTED(self):
            return self.getToken(SparksqlParser.UNCOMMITTED, 0)

        def UNKNOWN(self):
            return self.getToken(SparksqlParser.UNKNOWN, 0)

        def USING(self):
            return self.getToken(SparksqlParser.USING, 0)

        def VAR(self):
            return self.getToken(SparksqlParser.VAR, 0)

        def VARP(self):
            return self.getToken(SparksqlParser.VARP, 0)

        def VIEW_METADATA(self):
            return self.getToken(SparksqlParser.VIEW_METADATA, 0)

        def WORK(self):
            return self.getToken(SparksqlParser.WORK, 0)

        def XML(self):
            return self.getToken(SparksqlParser.XML, 0)

        def XMLNAMESPACES(self):
            return self.getToken(SparksqlParser.XMLNAMESPACES, 0)

        def getRuleIndex(self):
            return SparksqlParser.RULE_simple_id

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterSimple_id" ):
                listener.enterSimple_id(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitSimple_id" ):
                listener.exitSimple_id(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitSimple_id" ):
                return visitor.visitSimple_id(self)
            else:
                return visitor.visitChildren(self)




    def simple_id(self):

        localctx = SparksqlParser.Simple_idContext(self, self._ctx, self.state)
        self.enterRule(localctx, 88, self.RULE_simple_id)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 654
            _la = self._input.LA(1)
            if not(((((_la - 187)) & ~0x3f) == 0 and ((1 << (_la - 187)) & ((1 << (SparksqlParser.ABSOLUTE - 187)) | (1 << (SparksqlParser.APPLY - 187)) | (1 << (SparksqlParser.AUTO - 187)) | (1 << (SparksqlParser.AVG - 187)) | (1 << (SparksqlParser.BASE64 - 187)) | (1 << (SparksqlParser.CALLER - 187)) | (1 << (SparksqlParser.CAST - 187)) | (1 << (SparksqlParser.CATCH - 187)) | (1 << (SparksqlParser.CHECKSUM_AGG - 187)) | (1 << (SparksqlParser.COMMITTED - 187)) | (1 << (SparksqlParser.CONCAT - 187)) | (1 << (SparksqlParser.COOKIE - 187)) | (1 << (SparksqlParser.COUNT - 187)) | (1 << (SparksqlParser.COUNT_BIG - 187)) | (1 << (SparksqlParser.DELAY - 187)) | (1 << (SparksqlParser.DELETED - 187)) | (1 << (SparksqlParser.DENSE_RANK - 187)) | (1 << (SparksqlParser.DISABLE - 187)) | (1 << (SparksqlParser.DYNAMIC - 187)) | (1 << (SparksqlParser.ENCRYPTION - 187)) | (1 << (SparksqlParser.FAST - 187)) | (1 << (SparksqlParser.FAST_FORWARD - 187)) | (1 << (SparksqlParser.FIRST - 187)) | (1 << (SparksqlParser.FOLLOWING - 187)) | (1 << (SparksqlParser.FORWARD_ONLY - 187)) | (1 << (SparksqlParser.FULLSCAN - 187)) | (1 << (SparksqlParser.GLOBAL - 187)) | (1 << (SparksqlParser.GO - 187)) | (1 << (SparksqlParser.GROUPING - 187)) | (1 << (SparksqlParser.GROUPING_ID - 187)) | (1 << (SparksqlParser.HASH - 187)) | (1 << (SparksqlParser.INSENSITIVE - 187)) | (1 << (SparksqlParser.INSERTED - 187)) | (1 << (SparksqlParser.ISOLATION - 187)) | (1 << (SparksqlParser.KEEPFIXED - 187)) | (1 << (SparksqlParser.KEYSET - 187)) | (1 << (SparksqlParser.LAST - 187)) | (1 << (SparksqlParser.LEVEL - 187)) | (1 << (SparksqlParser.LOCAL - 187)) | (1 << (SparksqlParser.LOCK_ESCALATION - 187)) | (1 << (SparksqlParser.LOGIN - 187)) | (1 << (SparksqlParser.LOOP - 187)) | (1 << (SparksqlParser.MARK - 187)) | (1 << (SparksqlParser.MAX - 187)) | (1 << (SparksqlParser.MIN - 187)) | (1 << (SparksqlParser.MODIFY - 187)) | (1 << (SparksqlParser.NEXT - 187)) | (1 << (SparksqlParser.NAME - 187)) | (1 << (SparksqlParser.NOCOUNT - 187)) | (1 << (SparksqlParser.NOEXPAND - 187)) | (1 << (SparksqlParser.NORECOMPUTE - 187)) | (1 << (SparksqlParser.NTILE - 187)) | (1 << (SparksqlParser.NUMBER - 187)) | (1 << (SparksqlParser.OFFSET - 187)) | (1 << (SparksqlParser.ONLY - 187)) | (1 << (SparksqlParser.OPTIMISTIC - 187)) | (1 << (SparksqlParser.OPTIMIZE - 187)) | (1 << (SparksqlParser.OUT - 187)) | (1 << (SparksqlParser.OUTPUT - 187)) | (1 << (SparksqlParser.OWNER - 187)) | (1 << (SparksqlParser.PARTITION - 187)) | (1 << (SparksqlParser.PATH - 187)) | (1 << (SparksqlParser.PRECEDING - 187)) | (1 << (SparksqlParser.PRIOR - 187)))) != 0) or ((((_la - 251)) & ~0x3f) == 0 and ((1 << (_la - 251)) & ((1 << (SparksqlParser.RANGE - 251)) | (1 << (SparksqlParser.RANK - 251)) | (1 << (SparksqlParser.READONLY - 251)) | (1 << (SparksqlParser.READ_ONLY - 251)) | (1 << (SparksqlParser.RECOMPILE - 251)) | (1 << (SparksqlParser.RELATIVE - 251)) | (1 << (SparksqlParser.REMOTE - 251)) | (1 << (SparksqlParser.REPEATABLE - 251)) | (1 << (SparksqlParser.ROOT - 251)) | (1 << (SparksqlParser.ROW - 251)) | (1 << (SparksqlParser.ROWGUID - 251)) | (1 << (SparksqlParser.ROWS - 251)) | (1 << (SparksqlParser.ROW_NUMBER - 251)) | (1 << (SparksqlParser.SAMPLE - 251)) | (1 << (SparksqlParser.SCHEMABINDING - 251)) | (1 << (SparksqlParser.SCROLL - 251)) | (1 << (SparksqlParser.SCROLL_LOCKS - 251)) | (1 << (SparksqlParser.SELF - 251)) | (1 << (SparksqlParser.SERIALIZABLE - 251)) | (1 << (SparksqlParser.SNAPSHOT - 251)) | (1 << (SparksqlParser.STATIC - 251)) | (1 << (SparksqlParser.STATS_STREAM - 251)) | (1 << (SparksqlParser.STDEV - 251)) | (1 << (SparksqlParser.STDEVP - 251)) | (1 << (SparksqlParser.SUM - 251)) | (1 << (SparksqlParser.THROW - 251)) | (1 << (SparksqlParser.TIES - 251)) | (1 << (SparksqlParser.TIME - 251)) | (1 << (SparksqlParser.TRY - 251)) | (1 << (SparksqlParser.TYPE - 251)) | (1 << (SparksqlParser.TYPE_WARNING - 251)) | (1 << (SparksqlParser.UNBOUNDED - 251)) | (1 << (SparksqlParser.UNCOMMITTED - 251)) | (1 << (SparksqlParser.UNKNOWN - 251)) | (1 << (SparksqlParser.USING - 251)) | (1 << (SparksqlParser.VAR - 251)) | (1 << (SparksqlParser.VARP - 251)) | (1 << (SparksqlParser.VIEW_METADATA - 251)) | (1 << (SparksqlParser.WORK - 251)) | (1 << (SparksqlParser.XML - 251)) | (1 << (SparksqlParser.XMLNAMESPACES - 251)) | (1 << (SparksqlParser.ID - 251)))) != 0)):
                self._errHandler.recoverInline(self)
            else:
                self.consume()
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx

    class Null_notnullContext(ParserRuleContext):

        def __init__(self, parser, parent:ParserRuleContext=None, invokingState:int=-1):
            super().__init__(parent, invokingState)
            self.parser = parser

        def NULL(self):
            return self.getToken(SparksqlParser.NULL, 0)

        def NOT(self):
            return self.getToken(SparksqlParser.NOT, 0)

        def getRuleIndex(self):
            return SparksqlParser.RULE_null_notnull

        def enterRule(self, listener:ParseTreeListener):
            if hasattr( listener, "enterNull_notnull" ):
                listener.enterNull_notnull(self)

        def exitRule(self, listener:ParseTreeListener):
            if hasattr( listener, "exitNull_notnull" ):
                listener.exitNull_notnull(self)

        def accept(self, visitor:ParseTreeVisitor):
            if hasattr( visitor, "visitNull_notnull" ):
                return visitor.visitNull_notnull(self)
            else:
                return visitor.visitChildren(self)




    def null_notnull(self):

        localctx = SparksqlParser.Null_notnullContext(self, self._ctx, self.state)
        self.enterRule(localctx, 90, self.RULE_null_notnull)
        self._la = 0 # Token type
        try:
            self.enterOuterAlt(localctx, 1)
            self.state = 657
            _la = self._input.LA(1)
            if _la==SparksqlParser.NOT:
                self.state = 656
                self.match(SparksqlParser.NOT)


            self.state = 659
            self.match(SparksqlParser.NULL)
        except RecognitionException as re:
            localctx.exception = re
            self._errHandler.reportError(self, re)
            self._errHandler.recover(self, re)
        finally:
            self.exitRule()
        return localctx



    def sempred(self, localctx:RuleContext, ruleIndex:int, predIndex:int):
        if self._predicates == None:
            self._predicates = dict()
        self._predicates[28] = self.expression_sempred
        pred = self._predicates.get(ruleIndex, None)
        if pred is None:
            raise Exception("No predicate with index:" + str(ruleIndex))
        else:
            return pred(localctx, predIndex)

    def expression_sempred(self, localctx:ExpressionContext, predIndex:int):
            if predIndex == 0:
                return self.precpred(self._ctx, 4)
         

            if predIndex == 1:
                return self.precpred(self._ctx, 2)
         

            if predIndex == 2:
                return self.precpred(self._ctx, 1)
         

            if predIndex == 3:
                return self.precpred(self._ctx, 10)
         




